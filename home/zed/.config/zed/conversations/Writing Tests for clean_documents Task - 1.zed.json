{"id":"58776403-0df3-4b40-ba15-c8c8237a40f7","zed":"context","version":"0.4.0","text":"\n\n\nWrite tests for the test_clean_documents in a similar way to the test_clean_documents function.\nCertainly! To write tests for the `test_clean_documents` function, we need to understand what the `clean_documents` function does. Let's assume that `clean_documents` is a function that takes a list of documents (strings) and performs some cleaning operations such as removing punctuation, converting to lowercase, and removing stopwords.\n\nHere's an example of what the `clean_documents` function might look like:\n\n```python\nimport re\nfrom nltk.corpus import stopwords\n\ndef clean_documents(documents):\n    stop_words = set(stopwords.words('english'))\n    cleaned_docs = []\n    for doc in documents:\n        # Remove punctuation\n        doc = re.sub(r'[^\\w\\s]', '', doc)\n        # Convert to lowercase\n        doc = doc.lower()\n        # Remove stopwords\n        doc = ' '.join([word for word in doc.split() if word not in stop_words])\n        cleaned_docs.append(doc)\n    return cleaned_docs\n```\n\nNow, let's write tests for the `clean_documents` function. We'll use the `unittest` framework for this purpose.\n\n```python\nimport unittest\n\nclass TestCleanDocuments(unittest.TestCase):\n\n    def test_clean_documents(self):\n        documents = [\n            \"Hello, World!\",\n            \"This is a test document.\",\n            \"Another document, with punctuation!\"\n        ]\n        expected_output = [\n            \"hello world\",\n            \"test document\",\n            \"another document punctuation\"\n        ]\n        self.assertEqual(clean_documents(documents), expected_output)\n\n    def test_clean_documents_empty(self):\n        documents = []\n        expected_output = []\n        self.assertEqual(clean_documents(documents), expected_output)\n\n    def test_clean_documents_no_punctuation(self):\n        documents = [\n            \"Hello World\",\n            \"This is a test document\",\n            \"Another document with no punctuation\"\n        ]\n        expected_output = [\n            \"hello world\",\n            \"test document\",\n           \nWrite tests in the test_clean_documents.py for the clean_documents task in a similar way to the test_clean_documents function.\ndata-warehouse-workflows/main/collectors\n```py data-warehouse-workflows/main/collectors/__init__.py\nfrom collectors.config import get_config\nfrom collectors.custom_logging import setup_logging\nfrom collectors.telemetry import setup_telemetry\n\nconfig = get_config()\nsetup_logging(config)\nsetup_telemetry(config)\n```\n\n```py data-warehouse-workflows/main/collectors/collector_rss_workflow.py\nimport typing\n\nfrom flytekit import workflow\n\nfrom collectors.tasks.clean_documents import clean_documents\nfrom collectors.tasks.load_document_to_warehouse import load_document_to_warehouse\nfrom collectors.tasks.load_translation_to_warehouse import load_translation_to_warehouse\nfrom collectors.tasks.rss.extract_from_rss import extract_from_rss\nfrom collectors.tasks.rss.transform_rss import transform_rss\nfrom collectors.tasks.translate_documents import translate_documents\n\naggregator_type = \"rss\"\n\n\n@workflow\ndef collector_rss_workflow(\n    feed_url: str = \"https://www.chinanews.com.cn/rss/scroll-news.xml\",\n    collector_name: str = \"collector-name\",\n    batch_size: int = 100,\n    bucket_url: str = \"s3://adarga-osint-dev-1-data-warehouse\",\n    data_warehouse_grpc_endpoint: str = \"localhost:10147\",\n    collector_id: str = \"c009931c-776e-481f-8133-f269054dd463\",\n    collector_run_name: str = \"rss/c009931c-776e-481f-8133-f269054dd463/2006-01-02T00:00:00\",\n    last_successful_iteration: str = \"0001-01-01T00:00:00Z\",\n) -> typing.Dict[str, str]:\n    \"\"\"Workflow to collect documents from rss feeds and load them into the data-warehouse.\"\"\"\n\n    # Extract posts from collectors\n    rss_posts, last_successful_iteration = extract_from_rss(\n        feed_url=feed_url,\n        collector_name=collector_name,\n        collector_id=collector_id,\n        batch_size=batch_size,\n        collectors_root=\"collector\",\n        collector_run_name=collector_run_name,\n        bucket_url=bucket_url,\n        last_successful_iteration=last_successful_iteration,\n    )\n\n    # Transform the posts to documents\n    documents = transform_rss(\n        posts=rss_posts,\n        bucket_url=bucket_url,\n        collector_run_name=collector_run_name,\n        collector_id=collector_id,\n    )\n\n    # Load the documents into the database\n    saved_documents, saved_metadatas, _ = load_document_to_warehouse(\n        documents=documents,\n        grpc_endpoint=data_warehouse_grpc_endpoint,\n        bucket_url=bucket_url,\n        collector_id=collector_id,\n        collector_name=collector_name,\n        aggregator_type=aggregator_type,\n    )\n\n    cleaned_documents, _ = clean_documents(\n        documents=saved_documents,\n        metadatas=saved_metadatas,\n        bucket_url=bucket_url,\n    )\n    # Translate documents\n    translated_documents, updated_metadatas = translate_documents(\n        documents=cleaned_documents,\n        metadatas=saved_metadatas,\n        bucket_url=bucket_url,\n    )\n\n    load_translation_to_warehouse(\n        translated_documents=translated_documents,\n        translated_metadatas=updated_metadatas,\n        grpc_endpoint=data_warehouse_grpc_endpoint,\n        collector_id=collector_id,\n        collector_name=collector_name,\n        aggregator_type=aggregator_type,\n    )\n\n    return {\"last_successful_iteration\": last_successful_iteration}\n\n\nif __name__ == \"__main__\":\n    collector_rss_workflow()\n```\n\n```py data-warehouse-workflows/main/collectors/collector_webz_workflow.py\nimport typing\n\nfrom flytekit import workflow\n\nfrom collectors.config import get_config\nfrom collectors.tasks.clean_documents import clean_documents\nfrom collectors.tasks.load_document_to_warehouse import load_document_to_warehouse\nfrom collectors.tasks.load_translation_to_warehouse import load_translation_to_warehouse\nfrom collectors.tasks.translate_documents import translate_documents\nfrom collectors.tasks.webz.extract_from_webz import extract_from_webz\nfrom collectors.tasks.webz.transform_webz import transform_webz\n\naggregator_type = \"webz\"\nconfig = get_config()\n\n\n@workflow\ndef collector_webz_workflow(\n    query: str = \"\"\"\n    site:(\n        ria.ru OR chinadaily.com.cn OR china.org.cn OR sana.sy OR almanar.com.lb OR prnasia.com OR iarex.ru OR bel.ru\n        OR wday.ru OR imn.iq OR shine.cn OR businesswirechina.com OR anna-news.info OR asgam.com OR plataformamedia.com\n        OR monavista.ru OR alalam.ir OR almayadeen.net OR taxpravo.ru OR journal-neo.su OR alhurra.com OR newsgd.com\n        OR advis.ru OR alarabiya.net OR abamako.com OR bamada.net OR journaldumali.com OR lejalon.com OR malivox.net\n        OR malijet.com OR stolica-s.ru OR chinafrica.cn OR alsharqiya.com OR samaa.tv OR arabic.rt.com\n    )\"\"\",\n    collector_name: str = \"collector-name\",\n    params: str = '{\"highlight\":  true, \"size\": 100}',\n    batch_count: int = 100,\n    bucket_url: str = \"s3://adarga-osint-dev-1-data-warehouse\",\n    data_warehouse_grpc_endpoint: str = \"localhost:10147\",\n    collector_id: str = \"f57923dd-e92c-4d47-8a13-2035b4751a68\",\n    collector_run_name: str = \"webz/f57923dd-e92c-4d47-8a13-2035b4751a68/2006-01-02T00:00:00\",\n) -> typing.Dict[str, str]:\n    \"\"\"Workflow to collect documents from webz and load them into the data-warehouse.\"\"\"\n\n    # Extract posts from collectors\n    webz_posts = extract_from_webz(\n        query=query,\n        collector_name=collector_name,\n        collector_id=collector_id,\n        params=params,\n        batch_count=batch_count,\n        collectors_root=\"collector\",\n        collector_run_name=collector_run_name,\n        bucket_url=bucket_url,\n    )\n\n    # Transform the posts to documents of DataWarehouseDocument\n    documents = transform_webz(\n        posts=webz_posts,\n        bucket_url=bucket_url,\n        collector_run_name=collector_run_name,\n        collector_id=collector_id,\n    )\n\n    # Load the documents into the database\n    saved_documents, saved_metadatas, last_successful_iteration = load_document_to_warehouse(\n        documents=documents,\n        grpc_endpoint=data_warehouse_grpc_endpoint,\n        bucket_url=bucket_url,\n        collector_id=collector_id,\n        collector_name=collector_name,\n        aggregator_type=aggregator_type,\n    )\n\n    cleaned_documents, _ = clean_documents(\n        documents=saved_documents,\n        metadatas=saved_metadatas,\n        bucket_url=bucket_url,\n    )\n    # Translate documents\n    translated_documents, updated_metadatas = translate_documents(\n        documents=cleaned_documents,\n        metadatas=saved_metadatas,\n        bucket_url=bucket_url,\n    )\n\n    load_translation_to_warehouse(\n        translated_documents=translated_documents,\n        translated_metadatas=updated_metadatas,\n        grpc_endpoint=data_warehouse_grpc_endpoint,\n        collector_id=collector_id,\n        collector_name=collector_name,\n        aggregator_type=aggregator_type,\n    )\n\n    return {\"last_successful_iteration\": last_successful_iteration}\n\n\nif __name__ == \"__main__\":\n    collector_webz_workflow()\n```\n\n```py data-warehouse-workflows/main/collectors/config.py\nfrom functools import lru_cache\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Config(BaseSettings):\n    \"\"\"Configuration for the workflow\n\n    Using the BaseSettings class will automatically load from environment variables.\n    This means that defaults can be added here but we can override immediately by setting the environment variable.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\", env_file_encoding=\"utf-8\", env_ignore_empty=True, extra=\"ignore\", env_prefix=\"\"\n    )\n\n    APP_NAME: str = \"data-warehouse-workflows\"\n\n    LOG_LEVEL: str = \"INFO\"\n\n    ENVIRONMENT: str = \"PRODUCTION\"\n\n    # Whether translation is enabled or not\n    FEATURE_TRANSLATION_ENABLED: bool = False\n\n    # Chunk files for translation if file size exceeds LLM_MAX_CHAR_SIZE\n    LLM_CHUNK_FILES: bool = True\n\n    # OpenAI API key for initialising LLM translation provider\n    OPENAI_API_KEY: str = \"\"\n\n    # OpenAI base URL\n    OPENAI_BASE_URL: str = \"http://litellm-proxy.litellm:4000\"\n\n    # Which OpenAI model to use\n    OPENAI_MODEL: str = \"gpt-4o-mini\"\n\n    # The maximum number of tokens to use for translation of single chunk\n    TRANSLATION_MAX_LLM_CHAR_SIZE: int = 4096\n\n    # TARGET_LANGUAGE the target language to translate to\n    TARGET_LANGUAGE: str = \"en\"\n\n    # LANGUAGE_DETECTION_THRESHOLD the threshold for language detection\n    LANGUAGE_DETECTION_THRESHOLD: float = 0.5\n\n    # IS_TEST used to change the trace exported to an inmemory version\n    IS_TEST: bool = True\n\n\n@lru_cache\ndef get_config() -> Config:\n    \"\"\"\n    Get the config object and load only once.\n\n    :return: Config object\n    \"\"\"\n    return Config()\n```\n\n```py data-warehouse-workflows/main/collectors/custom_logging.py\nimport logging\nfrom functools import lru_cache\n\nimport structlog\nfrom opentelemetry import trace\nfrom opentelemetry.trace.span import format_span_id, format_trace_id\nfrom structlog.types import EventDict, Processor\n\nfrom collectors.config import Config\n\n\ndef rename_event_key(_, __, event_dict: EventDict) -> EventDict:\n    \"\"\"\n    Log entries keep the text message in the `event` field, but for consistency with our Go logs we want to use the\n    `msg` field. This processor moves the value from one field to the other.\n    \"\"\"\n    event_dict[\"msg\"] = event_dict.pop(\"event\")\n    return event_dict\n\n\ndef drop_color_message_key(_, __, event_dict: EventDict) -> EventDict:\n    \"\"\"\n    Uvicorn logs the message a second time in the extra `color_message`, but we don't\n    need it. This processor drops the key from the event dict if it exists.\n\n    Args:\n        _ : Unused positional argument.\n        __ : Unused positional argument.\n        event_dict (EventDict): The event dictionary containing log information.\n\n    Returns:\n        EventDict: The modified event dictionary without the `color_message` key.\n    \"\"\"\n    event_dict.pop(\"color_message\", None)\n    return event_dict\n\n\ndef tracer_injection(_, __, event_dict: EventDict) -> EventDict:\n    \"\"\"\n    Injects the current OTEL trace and span IDs into the event dictionary for logging.\n\n    Args:\n        _ : Unused positional argument.\n        __ : Unused positional argument.\n        event_dict (EventDict): The event dictionary containing log information.\n\n    Returns:\n        EventDict: The modified event dictionary with `trace_id` and `span_id` added.\n    \"\"\"\n    # get correlation ids from current tracer context\n    span_context = trace.get_current_span().get_span_context()\n    trace_id, span_id = (span_context.trace_id, span_context.span_id) if span_context else (None, None)\n\n    # add ids to structlog event dictionary. Use camelCase to match Go logs and make Loki queries easier\n    if trace_id and span_id:\n        event_dict[\"traceId\"] = format_trace_id(trace_id)\n        event_dict[\"spanId\"] = format_span_id(span_id)\n\n    return event_dict\n\n\ndef setup_logging(settings: Config) -> logging.Logger:\n    \"\"\"\n    Sets up structured logging using structlog and integrates it with the standard logging module.\n\n    Args:\n        settings (Settings): The logging level. Defaults to \"INFO\".\n    \"\"\"\n    shared_processors: list[Processor] = [\n        structlog.contextvars.merge_contextvars,\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.PositionalArgumentsFormatter(),\n        structlog.stdlib.ExtraAdder(),\n        drop_color_message_key,\n        tracer_injection,\n        structlog.processors.TimeStamper(fmt=\"iso\", key=\"time\"),\n        structlog.processors.StackInfoRenderer(),\n        rename_event_key,\n        structlog.processors.format_exc_info,\n    ]\n\n    structlog.configure(\n        processors=[\n            *shared_processors,\n            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,\n        ],\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    formatter = structlog.stdlib.ProcessorFormatter(\n        # These run ONLY on `logging` entries that do NOT originate within\n        # structlog.\n        foreign_pre_chain=shared_processors,\n        # These run on ALL entries after the pre_chain is done.\n        processors=[\n            # Remove _record & _from_structlog.\n            structlog.stdlib.ProcessorFormatter.remove_processors_meta,\n            structlog.processors.JSONRenderer(),\n        ],\n    )\n\n    handler = logging.StreamHandler()\n    # Use OUR `ProcessorFormatter` to format all `logging` entries.\n    handler.setFormatter(formatter)\n    root_logger = logging.getLogger()\n    root_logger.addHandler(handler)\n    root_logger.setLevel(settings.LOG_LEVEL.upper())\n\n    return root_logger\n\n\n@lru_cache\ndef get_logger() -> logging.Logger:\n    \"\"\"\n    Set up the logger and return it only once\n    \"\"\"\n    return logging.getLogger()\n```\n\ndatatypes\n```py data-warehouse-workflows/main/collectors/datatypes/__init__.py\n\n```\n\n```py data-warehouse-workflows/main/collectors/datatypes/document_models.py\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Timestamp(BaseModel):\n    seconds: int\n    nanos: int\n\n\nclass Attribute(BaseModel):\n    key: str\n    value_text: str = None\n    value_numeric: float = None\n    value_timestamp: Timestamp = None\n    value_text_array: list[str] = None\n\n\ndef find_attribute(attributes: list[Attribute], key: str) -> Optional[Attribute]:\n    for attr in attributes:\n        if attr.key == key:\n            return attr\n    return None\n\n\nclass DataWarehouseDocument(BaseModel):\n    source_aggregator: str\n    original_text: str\n    attributes: list[Attribute]\n\n\nclass UpdateDocumentTextRequest(BaseModel):\n    document_id: str\n    english_text: str\n\n\nclass DataLakeMetadata(BaseModel):\n    document_id: str\n    attributes: list[Attribute]\n\n\nclass WebZPost(BaseModel):\n    \"\"\"\n    WebZPost thread keys:\n    country (optional) - string\n    published - ISO datetime string\n    site_categories - array\n    site - string\n    title - string\n    uuid - string UUIDv4\n    url - string\n    \"\"\"\n\n    thread: dict\n    text: str\n    language: str\n    crawled: str\n\n\nclass RSSPost(BaseModel):\n    title: str\n    link: str\n    categories: list[str]\n    text: str\n    publishedAt: str  # noqa: N815\n```\n\n\nmapping\n```py data-warehouse-workflows/main/collectors/mapping/mapping.py\nimport pandas as pd\nimport pycountry\nfrom dateutil.parser import parse\n\nfrom collectors.datatypes.document_models import Timestamp\n\n\ndef convert_to_timestamp(date_string: str) -> Timestamp:\n    \"\"\"Convert date string to timestamp.\"\"\"\n    # Expects the date string is in the format \"2021-09-01T00:00:00Z\"\n\n    date_object = parse(date_string)\n    time = date_object.timestamp()\n    seconds = int(time)\n    nanos = int((time - seconds) * 10**9)\n    return Timestamp(seconds=seconds, nanos=nanos)\n\n\ndef get_country_from_domain(domain: str) -> str | None:\n    \"\"\"Map country from domain.\"\"\"\n    df = pd.read_csv(\"collectors/mapping/source_mapping_source_groups_v5.csv\")\n    entry = df.loc[df[\"source-URL\"] == domain]\n    if not entry.empty:\n        return get_country_code(entry[\"source-group-1-country-of-origin\"].values[0])\n\n    return None\n\n\ndef get_country_code(country_name: str) -> str:\n    \"\"\"Convert country to ISO 3166 two letter code.\"\"\"\n    try:\n        # pycountry returns none if it doesn't match any country\n        # check if the passed value is already a country code, then the name, then the common name\n        for attr in [\"alpha_2\", \"name\", \"common_name\"]:\n            country = pycountry.countries.get(**{attr: country_name})\n            if country:\n                return country.alpha_2\n        # if none of the above return a value, use fuzzy search\n        # eg. for names like `england` - returns United Kingdom\n        # use first hit (or just return None if multiple hits?)\n        countries = pycountry.countries.search_fuzzy(country_name)\n        if len(countries) > 0:\n            return countries[0].alpha_2\n    except AttributeError:\n        return None\n    except LookupError:\n        return None\n\n\ndef get_iso_language_code(language_name: str) -> str:\n    \"\"\"Convert language to ISO 639 two letter code.\"\"\"\n    try:\n        for attr in [\"alpha_2\", \"name\"]:\n            language = pycountry.languages.get(**{attr: language_name})\n            if language:\n                print(language)\n                return language.alpha_2\n    except AttributeError:\n        return None\n```\n\n```csv data-warehouse-workflows/main/collectors/mapping/source_mapping_source_groups_v5.csv\nsource-URL,source-groups-final,source-group-1-country-of-origin,source-group-2-internal-external,source-group-3-trusted-not-trusted,source-group-4-research-centres-govt-depts,source-group-5,source-group-6\narms-expo.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\ngmw.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\naa.com.tr,\"Turkey,External,Not trusted\",Turkey,External,Not trusted,None,,\nabcnews.go.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nadengulfnews.net,\"Yemen,External,Trusted\",Yemen,External,Trusted,None,,\nadsadvance.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\naerospace-technology.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\naf.mil,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nafghanistantimes.af,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\naf.reuters.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nafghanistan-analysts.org,\"Afghanistan,External,Trusted,Research centre\",Afghanistan,External,Trusted,Research centre,,\nafghanistannews.net,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\nafrica.chinadaily.com.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nafricafocus.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nafricaintelligence.com,\"France,External,Trusted\",France,External,Trusted,None,,\nafricanarguments.org,\"United Kingdom,External,Trusted,Research centre\",United Kingdom,External,Trusted,Research centre,,\nafricanews.com,\"France,External,Trusted\",France,External,Trusted,None,,\nafricanewsagency.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nafp.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nafrica-newsroom.com,\"Switzerland,External,Trusted\",Switzerland,External,Trusted,None,,\nafricaneyereport.com,\"Ghana,External,Trusted\",Ghana,External,Trusted,None,,\nafricanmanager.com,\"Tunisia,External,Trusted\",Tunisia,External,Trusted,None,,\nafricanreview.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nalbawaba.com,\"Jordan,External,Trusted\",Jordan,External,Trusted,None,,\nairforce-technology.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nairforcetimes.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nalarabiya.net,\"Saudi Arabia,External,Trusted\",Saudi Arabia,External,Trusted,None,,\naljazeera.com,\"Qatar,External,Trusted\",Qatar,External,Trusted,None,,\nallafrica.com ,\"United States,External,Trusted\",United States,External,Trusted,None,,\nal-monitor.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nalsumaria.tv,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nap.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\napanews.net,\"Senegal,External,Trusted\",Senegal,External,Trusted,None,,\napp.com.pk,\"Pakistan,External,Trusted\",Pakistan,External,Trusted,None,,\natlanticcouncil.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nara.reuters.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\narabianbusiness.com,\"United Arab Emirates,External,Trusted\",United Arab Emirates,External,Trusted,None,,\narabnews.com,\"Saudi Arabia,External,Trusted\",Saudi Arabia,External,Trusted,None,,\narmy.mod.uk,\"United Kingdom,External,Trusted,Government department\",United Kingdom,External,Trusted,Government department,,\narmy-technology.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nasia.nikkei.com,\"Japan,External,Trusted\",Japan,External,Trusted,None,,\nasiapacificdefencereporter.com,\"Australia,External,Trusted\",Australia,External,Trusted,None,,\naustraliandefence.com.au,\"Australia,External,Trusted\",Australia,External,Trusted,None,,\nbaltictimes.com,\"Latvia,External,Trusted\",Latvia,External,Trusted,None,,\nbbc.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nbellingcat.com,\"Netherlands,External,Trusted\",Netherlands,External,Trusted,None,,\nbleepingcomputer.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nbloomberg.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nbreakingdefense.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nbrookings.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nbusinessdailyafrica.com,\"Kenya,External,Trusted\",Kenya,External,Trusted,None,,\nbusinesstimes.com.sg,\"Singapore,External,Trusted\",Singapore,External,Trusted,None,,\nbusinessworld.in,\"India,External,Trusted\",India,External,Trusted,None,,\nc4isrnet.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncarnegie.ru,\"Russian Federation,External,Trusted,Research centre\",Russian Federation,External,Trusted,Research centre,,\ncarnegiecouncil.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncarnegieendowment.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncarnegieeurope.eu,\"Belgium,External,Trusted,Research centre\",Belgium,External,Trusted,Research centre,,\ncbc.ca,\"Canada,External,Trusted\",Canada,External,Trusted,None,,\ncfr.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nchannelnewsasia.com,\"Singapore,External,Trusted\",Singapore,External,Trusted,None,,\nchathamhouse.org,\"United Kingdom,External,Trusted,Research centre\",United Kingdom,External,Trusted,Research centre,,\ncnbc.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nctc.usma.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncnbcafrica.com,\"South Africa,External,Trusted\",South Africa,External,Trusted,None,,\ncounterterrorbusiness.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ncounteriedreport.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ncontracts.mod.uk,\"United Kingdom,External,Trusted,Government department\",United Kingdom,External,Trusted,Government department,,\ncrisisgroup.org,\"Belgium,External,Trusted,Research centre\",Belgium,External,Trusted,Research centre,,\ncriticalthreats.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncsis.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncsmonitor.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncyberdefensemagazine.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncybersecurityintelligence.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ndarkreading.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ndawn.com,\"Pakistan,External,Trusted\",Pakistan,External,Trusted,None,,\ndefence.pk,\"Pakistan,External,Not trusted\",Pakistan,External,Not trusted,None,,\ndefenceweb.co.za,\"South Africa,External,Trusted\",South Africa,External,Trusted,None,,\ndefenceconnect.com.au,\"Australia,External,Trusted\",Australia,External,Trusted,None,,\ndefenceonline.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ndefensenews.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ndefenceprocurementinternational.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ndefencetalk.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ndefensedaily.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ndefenseindustrydaily.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ndefenseone.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ndefensesystems.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ndefenseworld.net,\"India,External,Trusted\",India,External,Trusted,None,,\neuvsdisinfo.eu,\"Belgium,External,Trusted,Research centre\",Belgium,External,Trusted,Research centre,,\ndiplomaticobserver.com,\"Turkey,External,Not trusted\",Turkey,External,Not trusted,None,,\nedition.cnn.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncnn.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\neeas.europa.eu,\"Belgium,External,Trusted,Government department\",Belgium,External,Trusted,Government department,,\nehackingnews.com,\"Estonia,External,Trusted\",Estonia,External,Trusted,None,,\nen.afrik.com,\"France,External,Trusted\",France,External,Trusted,None,,\nen.mercopress.com,\"Uruguay,External,Trusted\",Uruguay,External,Trusted,None,,\neng.chinamil.com.cn,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\neuronews.com,\"France,External,Trusted\",France,External,Trusted,None,,\neuropol.europa.eu,\"The Netherlands,External,Trusted,Government department\",The Netherlands,External,Trusted,Government department,,\nforeignpolicy.com,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\neuro-sd.com,\"Germany,External,Trusted\",Germany,External,Trusted,None,,\neurosecglobal.de,\"Germany,External,Trusted\",Germany,External,Trusted,None,,\ngaroweonline.com,\"Somalia,External,Trusted\",Somalia,External,Trusted,None,,\nexpress.co.uk,\"United Kingdom,External,Not trusted\",United Kingdom,External,Not trusted,None,,\nexpresso.sapo.pt,\"Portugal,External,Trusted\",Portugal,External,Trusted,None,,\nfbi.org,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nfederalnewsnetwork.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nfifthdomain.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nforbes.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nforces.net,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nfrance24.com,\"France,External,Trusted\",France,External,Trusted,None,,\nfreedomhouse.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ngaroweonline.com,\"Somalia,External,Trusted\",Somalia,External,Trusted,None,,\ngcaptain.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nglobal.chinadaily.com.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\ngov.uk,\"United Kingdom,External,Trusted,Government department\",United Kingdom,External,Trusted,Government department,,\ngsn-online.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ngulfnews.com,\"United Arab Emirates,External,Trusted\",United Arab Emirates,External,Trusted,None,,\ngulf-times.com,\"Qatar,External,Trusted\",Qatar,External,Trusted,None,,\nhackread.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nhindustantimes.com,\"India,External,Trusted\",India,External,Trusted,None,,\nhomelandsecuritynewswire.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ntheintercept.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nicds.ee,\"Estonia,External,Trusted,Research centre\",Estonia,External,Trusted,Research centre,,\nhudson.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nhuffingtonpost.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nibtimes.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nindianexpress.com,\"India,External,Trusted\",India,External,Trusted,None,,\njihadology.net,\"United States,External,Trusted\",United States,External,Trusted,None,,\njordantimes.com,\"Jordan,External,Trusted\",Jordan,External,Trusted,None,,\ninforisktoday.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ninfosecurity-magazine.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nkhaama.com,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\ninterfax.com,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\ninterpol.int,\"France,External,Trusted,Government department\",France,External,Trusted,Government department,,\nmercopress.com,\"Uruguay,External,Trusted\",Uruguay,External,Trusted,None,,\niraq-businessnews.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\niraqdailyjournal.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\niraqinews.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\niss.europa.eu,\"France,External,Trusted,Research centre\",France,External,Trusted,Research centre,,\nissafrica.org,\"South Africa,External,Trusted,Research centre\",South Africa,External,Trusted,Research centre,,\njamestown.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\njanes.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\njordantimes.com,\"Jordan,External,Trusted\",Jordan,External,Trusted,None,,\njournaldumali.com,\"Mali,External,Trusted\",Mali,External,Trusted,None,,\njuancole.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nkabulpress.org,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\nkhaleejtimes.com,\"United Arab Emirates,External,Trusted\",United Arab Emirates,External,Trusted,None,,\nkrebsonsecurity.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nlongwarjournal.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nmei.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nmiddleeasteye.net,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nmiddleeastmonitor.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nosce.org,\"Austria,External,Trusted,Government department\",Austria,External,Trusted,Government department,,\nmiddle-east-online.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nmilitary.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nmilitarytimes.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nnakedsecurity.sophos.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\npewresearch.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nnationaldefensemagazine.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nnationalgeographic.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nnato.int,\"Belgium,External,Trusted,Government department\",Belgium,External,Trusted,Government department,,\nrand.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nnaval-technology.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nnavyrecognition.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nnews.hitb.org,\"Malaysia,External,Trusted\",Malaysia,External,Trusted,None,,\nnews.un.org,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nnews24.com,\"South Africa,External,Trusted\",South Africa,External,Trusted,None,,\nnti.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nnytimes.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\noccrp.org,\"Romania,External,Trusted\",Romania,External,Trusted,None,,\nOECD.org,\"France,External,Trusted,Research centre\",France,External,Trusted,Research centre,,\nohchr.org,\"France,External,Trusted,Government department\",France,External,Trusted,Government department,,\nopendemocracy.net,\"United Kingdom,External,Trusted,Research centre\",United Kingdom,External,Trusted,Research centre,,\nosac.gov,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\npajhwok.com,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\npakobserver.net,\"Pakistan,External,Trusted\",Pakistan,External,Trusted,None,,\nparliament.uk,\"United Kingdom,External,Trusted,Government department\",United Kingdom,External,Trusted,Government department,,\npolitico.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nraf.mod.uk,\"United Kingdom,External,Trusted,Government department\",United Kingdom,External,Trusted,Government department,,\nsipri.org,\"Sweden,External,Trusted,Research centre\",Sweden,External,Trusted,Research centre,,\ntass.com,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nreliefweb.int,\"United States,External,Trusted\",United States,External,Trusted,None,,\nreuters.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nterrorismwatch.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\ntheafricareport.com,\"France,External,Trusted\",France,External,Trusted,None,,\nrferl.org,\"Czech Republic,External,Trusted\",Czech Republic,External,Trusted,None,,\nroyalnavy.mod.uk,\"United Kingdom,External,Trusted,Government department\",United Kingdom,External,Trusted,Government department,,\nrt.com,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nrthk.hk,\"Hong Kong,External,Trusted\",Hong Kong,External,Trusted,None,,\nruaviation.com,\"Russian Federation,External,Trusted\",Russian Federation,External,Trusted,None,,\nrudaw.net,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nrusi.org,\"United Kingdom,External,Trusted,Research centre\",United Kingdom,External,Trusted,Research centre,,\nsaharamedias.net,\"Mauritania ,External,Trusted\",Mauritania ,External,Trusted,None,,\nsaharareporters.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nsaudigazette.com.sa,\"Saudi Arabia,External,Trusted\",Saudi Arabia,External,Trusted,None,,\nscmp.com,\"China,External,Not trusted\",China,External,Not trusted,None,,\nsdg.iisd.org,\"Canada,External,Trusted,Research centre\",Canada,External,Trusted,Research centre,,\ntimesofisrael.com,\"Israel,External,Trusted\",Israel,External,Trusted,None,,\nsecurityinfowatch.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nsecurityintelligence.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nsierraleonetimes.com,\"Sierra Leone,External,Trusted\",Sierra Leone,External,Trusted,None,,\nsecuritycouncilreport.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nslateafrique.com,\"France,External,Trusted\",France,External,Trusted,None,,\nsomalilandsun.com,\"Somalia,External,Trusted\",Somalia,External,Trusted,None,,\nsputniknews.com,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nstate.gov,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\ntechcrunch.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nwarfare.today,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ntehrantimes.com,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nwikileaks.org,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nthebaghdadpost.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nthediplomat.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ntheglobalobservatory.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ntheguardian.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nthehackernews.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nthemiddlemarket.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncounterextremism.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nthemiddlemarket.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nthemoscowtimes.com,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nthenews.com.pk,\"Pakistan,External,Trusted\",Pakistan,External,Trusted,None,,\nthinkdefence.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nthreatpost.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ntimesofindia.indiatimes.com,\"India,External,Not trusted\",India,External,Not trusted,None,,\ntolonews.com,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\ntribuneindia.com,\"India,External,Trusted\",India,External,Trusted,None,,\nsahel-intelligence.com,\"France,External,Trusted\",France,External,Trusted,None,,\nukdefencejournal.org.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nunderstandingwar.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nunodc.org,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nupi.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\npolygraph.info,\"United States,External,Trusted\",United States,External,Trusted,None,,\ninformnapalm.org,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nnavalnews.com,\"France,External,Trusted\",France,External,Trusted,None,,\nsavetherhino.org,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ntraffic.org,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nvoanews.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nwam.ae,\"United Arab Emirates,External,Trusted\",United Arab Emirates,External,Trusted,None,,\nwarontherocks.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nwashingtoninstitute.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncites.org,\"Switzerland,External,Trusted\",Switzerland,External,Trusted,None,,\nwhatsinblue.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nmil.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\ncrimea.gov.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\necho.msk.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nvedomosti.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nfsvts.gov.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nyalibnan.com,\"Lebanon,External,Trusted\",Lebanon,External,Trusted,None,,\nalsharqiya.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\niraqnewsgazette.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nshafaaq.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nninanews.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\ngov.krd,\"Iraq,External,Trusted,Government department\",Iraq,External,Trusted,Government department,,\nthebaghdadpost.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\ntheiranproject.com,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nir.usembassy.gov,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nsyrianewsgazette.com,\"Syria,External,Not trusted\",Syria,External,Not trusted,None,,\nsana.sy,\"Syria,External,Not trusted\",Syria,External,Not trusted,None,,\ndailystar.com.lb,\"Lebanon,External,Trusted\",Lebanon,External,Trusted,None,,\nalriyadh.com,\"Saudi Arabia,External,Trusted\",Saudi Arabia,External,Trusted,None,,\nalarabiya.net,\"Saudi Arabia,External,Trusted\",Saudi Arabia,External,Trusted,None,,\nalmasdarnews.com,\"Syria ,External,Not trusted\",Syria ,External,Not trusted,None,,\naps.dz,\"Algeria,External,Trusted\",Algeria,External,Trusted,None,,\naopnews.com,\"Afghanistan,External,Trusted\",Afghanistan,External,Trusted,None,,\nlibyaherald.com,\"Libya,External,Trusted\",Libya,External,Trusted,None,,\nalmusallh.ly,\"Libya,External,Trusted\",Libya,External,Trusted,None,,\nmeforum.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nen.arij.net,\"Jordan,External,Trusted\",Jordan,External,Trusted,None,,\ntasnimnews.com,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nbasijnews.ir,\"Iran,External,Not trusted,Government department\",Iran,External,Not trusted,Government department,,\nnewsyemen.net,\"Yemen,External,Trusted\",Yemen,External,Trusted,None,,\nalsabaah.iq,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nimn.iq,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nsaidshuhada.com,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\niranwire.com,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\niranpressnews.com,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nhispantv.com,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nsyrianownews.com,\"Syria,External,Not trusted\",Syria,External,Not trusted,None,,\nyoum7.com,\"Egypt,External,Trusted\",Egypt,External,Trusted,None,,\ndailybeirut.com,\"Lebanon,External,Trusted\",Lebanon,External,Trusted,None,,\nalittihad.ae,\"United Arab Emirates,External,Trusted\",United Arab Emirates,External,Trusted,None,,\nalrai.com,\"Jordan,External,Trusted\",Jordan,External,Trusted,None,,\nalquds.com,\"United Kingdom,External,Not trusted\",United Kingdom,External,Not trusted,None,,\nsirapost.com,\"Yemen,External,Trusted\",Yemen,External,Trusted,None,,\nakhbarelwatane.net,\"Algeria,External,Trusted\",Algeria,External,Trusted,None,,\nalwasat.ly,\"Libya,External,Trusted\",Libya,External,Trusted,None,,\nradionationale.tn,\"Tunisia,External,Trusted\",Tunisia,External,Trusted,None,,\nmashreghnews.ir,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\njamejamonline.ir,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nbasijnews.ir,\"Iran,External,Not trusted\",Iran,External,Not trusted,,,\nhamshahrionline.ir,\"Iran,External,Not trusted\",Iran,External,Not trusted,None,,\nmofa.gov.sa,\"Saudi Arabia,External,Not trusted,Government department\",Saudi Arabia,External,Not trusted,Government department,,\naksalser.com,\"Syria,External,Not trusted\",Syria,External,Not trusted,None,,\nmoi.gov.sy,\"Syria,External,Not trusted,Government department\",Syria,External,Not trusted,Government department,,\nzamanalwsl.net,\"Syria,External,Not trusted\",Syria,External,Not trusted,None,,\nalmustakbalpaper.net,\"Iraq,External,Trusted\",Iraq,External,Trusted,None,,\nalhadath-yemen.com,\"Yemen,External,Trusted\",Yemen,External,Trusted,None,,\nuschina.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nnavalnews.com,\"France,External,Trusted\",France,External,Trusted,None,,\nrand.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nlexingtoninstitute.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nwarontherocks.com,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncrss.pk,\"Pakistan,External,Trusted,Research centre\",Pakistan,External,Trusted,Research centre,,\nwavellroom.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nforeignpolicy.com,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nfromthegreennotebook.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nthinkdefence.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nbrookings.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nbelfercenter.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncarnegieendowment.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\natlanticcouncil.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\ncfr.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nheritage.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nswp-berlin.org,\"Germany,External,Trusted,Research centre\",Germany,External,Trusted,Research centre,,\niai.it,\"Italy,External,Trusted,Research centre\",Italy,External,Trusted,Research centre,,\nstimson.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nhudson.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nndu.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nsmallwarsjournal.com,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\njamestown.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nusma.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nbruegel.org,\"Belgium,External,Trusted,Research centre\",Belgium,External,Trusted,Research centre,,\nforeignaffairs.com,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nglobalsecurity.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nlongwarjournal.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nunderstandingwar.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nsecuritycouncilreport.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nukdefencejournal.org.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nasianmilitaryreview.com,\"Thailand,External,Trusted\",Thailand,External,Trusted,None,,\nmilitaryleak.com,\"Singapore,External,Trusted\",Singapore,External,Trusted,None,,\nmerics.org,\"Germany,External,Trusted,Research centre\",Germany,External,Trusted,Research centre,,\nopex360.com,\"France,External,Trusted\",France,External,Trusted,None,,\nstratfor.com,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nindiandefensenews.in,\"India,External,Trusted\",India,External,Trusted,None,,\nednews.net,\"Azerbaijan,External,Not trusted\",Azerbaijan,External,Not trusted,None,,\nthebulletin.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nseapowermagazine.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nmoderndiplomacy.eu,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nchinadialogue.net,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nhongkongfp.com,\"Hong Kong,External,Trusted\",Hong Kong,External,Trusted,None,,\nmarinelink.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nmaritimeglobalnews.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nnavy.mil,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nsafety4sea.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\ndmilt.com,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nshippingwatch.com,\"Denmark,External,Trusted\",Denmark,External,Trusted,None,,\ndefencealert.com,\"Australia,External,Trusted\",Australia,External,Trusted,None,,\nxinhuanet.com,\"China,External,Not trusted\",China,External,Not trusted,None,,\nchinadaily.com.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nfocustaiwan.tw,\"China,External,Trusted\",China,External,Trusted,None,,\nsinopsis.cz,\"Czech Republic,External,Trusted,Research centre\",Czech Republic,External,Trusted,Research centre,,\nchinadigitaltimes.net,\"United States,External,Trusted\",United States,External,Trusted,None,,\nlowyinstitute.org,\"Australia,External,Trusted,Research centre\",Australia,External,Trusted,Research centre,,\ndefencereviewasia.com,\"Singapore,External,Trusted\",Singapore,External,Trusted,None,,\nthejakartapost.com,\"Indonesia ,External,Trusted\",Indonesia ,External,Trusted,None,,\nthinkchina.sg,\"Singapore,External,Trusted,Research centre\",Singapore,External,Trusted,Research centre,,\neastasiaforum.org,\"Australia,External,Trusted,Research centre\",Australia,External,Trusted,Research centre,,\nchina-briefing.com,\"China,External,Not trusted\",China,External,Not trusted,None,,\ncarnegietsinghua.org,\"China,External,Trusted,Research centre\",China,External,Trusted,Research centre,,\npeople.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nchinaafricarealstory.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nchinamediaproject.org,\"Hong Kong,External,Trusted\",Hong Kong,External,Trusted,None,,\nvestnik-rm.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\narms-expo.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nlevada.ru,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nsova-center.ru,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nng.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nx-true.info,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nhse.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\npircenter.org,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nvaldaiclub.com,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nRussian Federationncouncil.ru,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nmid.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nfsb.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nRussian Federationnforces.org,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nkremlin.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\ngov.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\ngovernment.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nasi.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nrusemb.org.uk,\"Russian Federation ,External,Not trusted,Government department\",Russian Federation ,External,Not trusted,Government department,,\nchengdu.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\n81.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nhsw.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nlegaldaily.com.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\ngmw.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nthepaper.cn,\"China,External,Not trusted\",China,External,Not trusted,None,,\nkavkasia.net,\"Georgia,External,Not trusted\",Georgia,External,Not trusted,None,,\nzelao.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nria.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\n1rre.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\n23rus.org,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nmil.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\ntheins.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\neurasian-defence.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nmosmonitor.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nkamchat.info,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nmediafax.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nroscosmos.ru,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nrg.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nvestnik-glonass.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nscio.gov.cn,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nmod.gov.cn,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nindiatimes.com,\"India,External,Not trusted\",India,External,Not trusted,None,,\nindiatoday.in,\"India,External,Not trusted\",India,External,Not trusted,None,,\nIndianexpress.com,\"India,External,Not trusted\",India,External,Not trusted,None,,\nfreepressjournal.in,\"India,External,Not trusted\",India,External,Not trusted,None,,\nukrinform.net,\"Ukraine,External,Not trusted\",Ukraine,External,Not trusted,None,,\nunian.info,\"Ukraine,External,Not trusted\",Ukraine,External,Not trusted,None,,\nukranews.com,\"Ukraine,External,Not trusted\",Ukraine,External,Not trusted,None,,\neuromaidanpress.com,\"Ukraine,External,Not trusted\",Ukraine,External,Not trusted,None,,\ninterfax.com.ua,\"Ukraine,External,Not trusted\",Ukraine,External,Not trusted,None,,\nukrainetoday.org,\"Ukraine,External,Not trusted\",Ukraine,External,Not trusted,None,,\ntapeitimes.com,\"China,External,Not trusted\",China,External,Not trusted,None,,\ntaiwannews.com.tw,\"China,External,Not trusted\",China,External,Not trusted,None,,\ndigitimes.com,\"China,External,Not trusted\",China,External,Not trusted,None,,\nthenewslens.com,\"China,External,Not trusted\",China,External,Not trusted,None,,\nfocustaiwan.tw,\"China,External,Not trusted\",China,External,Not trusted,None,,\nAnalytical Center for the Government of the Russian Federationn Federation,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nCAST,\"Russian Federation,External,Trusted,Research centre\",Russian Federation,External,Trusted,Research centre,,\nAcademy of Military Science ,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nNatsional'nyy Issledovatel'skiy Institut Mirovoy Ekonomiki I Mezhdunarodnykh Otnosheniy Imeni E. M. Primakova,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nAero Engine Corporation of China ,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nInternational College of Defence Studies ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nThe PLA Information Engineering University ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Aerospace Science and Industry Corporation,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nBeijing Academy of Artificial Intelligence ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina South Industries Group Corporation,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nMinistry of Defence of the Russian Federationn Federation ,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nYaderny Kontrol,\"Russian Federation,External,Trusted,Research centre\",Russian Federation,External,Trusted,Research centre,,\nPermament Mission of the PRC to the United Nations at Geneva and other international organizations in swizterland ,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nChina Institute for International Strategic Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nShanghai Institute for International Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nICT Moscow,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nCentre for China and Globalization,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nThe Charhar Institute,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Development Institute,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\n\"Chongyang Institute for Financial Studies, Renmin University of China \",\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nDevelopment Research Center of The State Council ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nThe Centre of International and Regional Policy (CIRP) St Petersburg,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nThe Association of European Studies (AES),\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nThe Energy Foundation China,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChinese Academy of Social Science ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nWuhan University Center of Economic Diplomacy,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nShanghai Academy of Social Science ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nHong Kong Policy Research Institute,\"Hong Kong,External,Trusted,Research centre\",Hong Kong,External,Trusted,Research centre,,\nHKPRI International Relations Center,\"Hong Kong,External,Trusted,Research centre\",Hong Kong,External,Trusted,Research centre,,\nChina Military Online ,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nIfeng,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nCommunist Party Portal,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nAcademic Committeeof the Chinese Academy of Management Sciences ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Institute for Reform and Development ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nNational University of Defense Technology China,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nJournal of National University of Defense Technology China,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nFudan University Institute of International Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nVolunteer Army Research Centre,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Institute of Contemporary International Relations,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nWuhan University Institute of International Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nMaritime Silk Road Research Center (at the Guangdong Academy of Social Sciences),\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Association of Development Strategy Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Institute for Innovation and Development Strategy ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChinese Academy of Science and Technology for Development ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nCfisnet,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nBelt and Road Studies Network,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\n\"Tongji University, School of Political Science and International Relations \",\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nJilin University Institute of International Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nPublic Opinion Strategy Research Centre,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nBeijing Institute of Strategy and Management ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nMilitary Communications Engine Room and Data Center,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nCITIC Foundation for Reform and Development Studies ,\"China ,External,Not trusted,Research centre\",China ,External,Not trusted,Research centre,,\nChina Centre for International Startegic Studies ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nLibrary of the Academy of Military Sciences,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nHuaro Technology Co,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChinese Society for Artificial Intelligence,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChinese Institute of Command and Control,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nAuthoritative National Defense Science and Technology Information Portal,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nChina Simulation Federation,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nPekin University's China Center for Strategic Studies,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nJinsitan ,\"China,External,Not trusted,Research centre\",China,External,Not trusted,Research centre,,\nCenter for Military-Strategic Research of the Military Academy of the General Staff of the Armed Forces of the Russian Federationn Federation,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\n\"Institute for the US and Canadian Studies, Russian Federationn Academy of Sciences (ISKRAN)\",\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nCenter for Military-Strategic Research JSC of the Ministry of Defense of the Republic of Kazakhstan,\"Kazakhstan,External,Not trusted,Research centre\",Kazakhstan,External,Not trusted,Research centre,,\nRussian Federationn Institute for Strategic Thinking ,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nResearch Institute for Strategic Planning ,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nInstitute for National Security Strategy ,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nThe BelaRussian Federationn Institute for Strategic Studies (BISS),\"Belarus,External,Not trusted,Research centre\",Belarus,External,Not trusted,Research centre,,\nNational Institute for Strategic Development anf Risk Management ,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nInstitute of Socio-Economic Strategies and Development Technologies,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nThe Institute for Economic Strategies of the Department of Social Sciences of the Russian Federationn Academy of Sciences,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nThe National Institute for Strategic Studies of the Kyrgyz Republic,\"Kyrgyz Republic,External,Not trusted,Research centre\",Kyrgyz Republic,External,Not trusted,Research centre,,\ninteraffairs.ru,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\nRussian Federationn International Affairs Journal,\"Russian Federation,External,Not trusted,Research centre\",Russian Federation,External,Not trusted,Research centre,,\nnation.co.ke,\"Kenya,External,Trusted\",Kenya,External,Trusted,None,,\nfbi.gov,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nsecurityinformed.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncimsec.org,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nthe-star.co.ke,\"Kenya,External,Trusted\",Kenya,External,Trusted,None,,\nec.europa.eu,\"Belgium,External,Trusted,Government department\",Belgium,External,Trusted,Government department,,\nthechinabizz.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\ncapitalfm.co.ke,\"Kenya,External,Trusted\",Kenya,External,Trusted,None,,\nlawfareblog.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\npresident.go.ke,\"Kenya,External,Trusted,Government department\",Kenya,External,Trusted,Government department,,\nmonitoring.bbc.co.uk,\"United Kingdom,External,Trusted\",United Kingdom,External,Trusted,None,,\nchinamil.com.cn,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nfederaltimes.com,\"United States,External,Trusted\",United States,External,Trusted,None,,\nactualidad.rt.com,\"Russian Federation,External,Not trusted\",Russian Federation,External,Not trusted,None,,\npraguemonitor.com,\"Czechia,External,Trusted\",Czechia,External,Trusted,None,,\nimeu.org,\"United States,External,Trusted\",United States,External,Trusted,None,,\nafricanewsagency.fr,\"France,External,Trusted\",France,External,Trusted,None,,\nmwi.usma.edu,\"United States,External,Trusted,Research centre\",United States,External,Trusted,Research centre,,\nstandardmedia.co.ke,\"Kenya,External,Trusted\",Kenya,External,Trusted,None,,\nkbc.co.ke,\"Kenya,External,Trusted\",Kenya,External,Trusted,None,,\nimf.org,\"United States,External,Trusted,Government department\",United States,External,Trusted,Government department,,\nshafaq.com,\"Iraq,External,Not trusted\",Iraq,External,Not trusted,None,,\nbulkupload,\"United Kingdom,Internal,Trusted,Government department\",United Kingdom,Internal,Trusted,Government department,,\nPeople's Liberation Army,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\nMinistry of Defence of the Russian Federationn Federation Doctrine,\"Russian Federation,External,Not trusted,Government department\",Russian Federation,External,Not trusted,Government department,,\nPeople's Liberation Army Doctrine,\"China,External,Not trusted,Government department\",China,External,Not trusted,Government department,,\n```\n\n\n```py data-warehouse-workflows/main/collectors/metrics.py\nimport os\nimport urllib.error\nfrom typing import Optional\n\nfrom prometheus_client import CollectorRegistry, Counter, Gauge, pushadd_to_gateway\n\nfrom collectors.utils import JsonLog\n\n\ndef update_metrics(num_successful: int, num_failed: int, collector_id: str, collector_name: str, aggregator_type: str):\n    gateway_address = os.environ.get(\"PUSH_GATEWAY_ADDRESS\", None)\n    if gateway_address is not None:\n        label_values = dict(aggregatorType=aggregator_type, collectorID=collector_id, collectorName=collector_name)\n        label_names = list(label_values.keys())\n\n        registry = CollectorRegistry()\n        metrics_prefix = \"adarga_data_warehouse_workflows\"\n\n        success_counter = Counter(\n            f\"{metrics_prefix}_num_documents_successfully_uploaded\",\n            \"Number of documents successfully uploaded\",\n            label_names,\n            registry=registry,\n        )\n        success_counter.labels(**label_values).inc(num_successful)\n\n        failed_counter = Counter(\n            f\"{metrics_prefix}_num_documents_failed_to_upload\",\n            \"Number of documents failed to upload\",\n            label_names,\n            registry=registry,\n        )\n        failed_counter.labels(**label_values).inc(num_failed)\n\n        pushadd_to_gateway_retry(gateway_address, job=\"data-warehouse\", registry=registry)\n\n\ndef push_api_quota(api_quota: float, aggregator_type: str):\n    gateway_address = os.environ.get(\"PUSH_GATEWAY_ADDRESS\", None)\n    if gateway_address is not None:\n        print(\"setting quota\")\n        label_values = dict(aggregatorType=aggregator_type)\n        label_names = list(label_values.keys())\n\n        registry = CollectorRegistry()\n        metrics_prefix = \"adarga_data_warehouse_workflows\"\n\n        api_quota_guage = Gauge(\n            f\"{metrics_prefix}_api_quota_remaining\",\n            \"Remaining API quota as reported by the external API\",\n            label_names,\n            registry=registry,\n        )\n        api_quota_guage.labels(**label_values).set(api_quota)\n\n        pushadd_to_gateway_retry(gateway_address, job=\"data-warehouse\", registry=registry)\n\n\ndef push_backlog(num_backlog: float, collector_id: str, collector_name: str, aggregator_type: str):\n    gateway_address = os.environ.get(\"PUSH_GATEWAY_ADDRESS\", None)\n    if gateway_address is not None:\n        label_values = dict(aggregatorType=aggregator_type, collectorID=collector_id, collectorName=collector_name)\n        label_names = list(label_values.keys())\n\n        registry = CollectorRegistry()\n        metrics_prefix = \"adarga_data_warehouse_workflows\"\n\n        api_quota_guage = Gauge(\n            f\"{metrics_prefix}_num_backlog_articles\",\n            \"Number of articles remaining available to fetch\",\n            label_names,\n            registry=registry,\n        )\n        api_quota_guage.labels(**label_values).set(num_backlog)\n\n        pushadd_to_gateway_retry(gateway_address, job=\"data-warehouse\", registry=registry)\n\n\ndef dec_backlog(collector_id: str, collector_name: str, aggregator_type: str):\n    gateway_address = os.environ.get(\"PUSH_GATEWAY_ADDRESS\", None)\n    if gateway_address is not None:\n        label_values = dict(aggregatorType=aggregator_type, collectorID=collector_id, collectorName=collector_name)\n        label_names = list(label_values.keys())\n\n        registry = CollectorRegistry()\n        metrics_prefix = \"adarga_data_warehouse_workflows\"\n\n        api_quota_guage = Gauge(\n            f\"{metrics_prefix}_num_backlog_articles\",\n            \"Number of articles remaining available to fetch\",\n            label_names,\n            registry=registry,\n        )\n        api_quota_guage.labels(**label_values).dec()\n\n\ndef pushadd_to_gateway_retry(\n    gateway: str,\n    job: str,\n    registry: Optional[CollectorRegistry],\n) -> None:\n    \"\"\"Wraps the pushadd_to_gateway with a retry, catches and logs errors\"\"\"\n    retries = 3\n    while retries > 0:\n        try:\n            pushadd_to_gateway(gateway, job=job, registry=registry)\n            return\n        except urllib.error.URLError as e:\n            print(JsonLog(msg=f\"Error pushing to gateway: {e}\"))\n        retries -= 1\n```\n\n```py data-warehouse-workflows/main/collectors/process_run.py\nfrom flytekit import workflow\n\nfrom collectors.tasks.get_documents_from_collector import get_documents_from_collector\nfrom collectors.tasks.load_document_to_warehouse import load_document_to_warehouse\nfrom collectors.tasks.transform_documents import transform_documents\n\n\n@workflow\ndef process_run(\n    data_warehouse_bucket_url: str = \"s3://adarga-osint-dev-1-data-warehouse\",\n    data_warehouse_grpc_endpoint: str = \"localhost:10076\",\n    collectors_root: str = \"collector\",\n    collector_run_name: str = \"aggregator-type/collector-name/run-timestamp\",\n    collector_id: str = \"f57923dd-e92c-4d47-8a13-2035b4751a68\",\n    collector_name: str = \"collector-name\",\n    aggregator_type: str = \"webz\",\n):\n    \"\"\"Processes documents that have been collected by osint-management-service but not yet processed.\"\"\"\n\n    # Get list of previously collected webz posts from s3\n    posts = get_documents_from_collector(\n        bucket_url=data_warehouse_bucket_url,\n        collectors_root=collectors_root,\n        collector_run_name=collector_run_name,\n    )\n\n    # Transform the posts to documents\n    documents = transform_documents(\n        posts=posts,\n        bucket_url=data_warehouse_bucket_url,\n        collector_run_name=collector_run_name,\n        collector_id=collector_id,\n        aggregator_type=aggregator_type,\n    )\n\n    # Load the documents into the database\n    load_document_to_warehouse(\n        documents=documents,\n        grpc_endpoint=data_warehouse_grpc_endpoint,\n        bucket_url=data_warehouse_bucket_url,\n        collector_id=collector_id,\n        collector_name=collector_name,\n        aggregator_type=aggregator_type,\n    )\n\n\nif __name__ == \"__main__\":\n    process_run()\n```\n\ntasks\n```py data-warehouse-workflows/main/collectors/tasks/__init__.py\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/clean_documents.py\nimport logging\nimport typing\n\nfrom flytekit import FlyteFile, task\n\nfrom collectors.config import get_config\nfrom collectors.datatypes.document_models import DataLakeMetadata\nfrom collectors.tasks.text_cleaning.cleaning import clean_document_text\nfrom collectors.utils import flyte_file_to_input, raw_file_to_flyte\n\nconfig = get_config()\nlogger = logging.getLogger()\n\nfile_prefix = \"data-warehouse\"\n\n\n@task(pod_template_name=\"data-warehouse-pod-template\")\ndef clean_documents(\n    documents: typing.List[FlyteFile], metadatas: typing.List[FlyteFile], bucket_url: str\n) -> typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile]]:\n    \"\"\"\n    Tries to clean the documents.  Returns the original if it fails\n\n    Args:\n        documents: typing.List[FlyteFile] - typing.List of the document contents\n        metadatas: typing.List[FlyteFile] - typing.List of the document metadata\n        bucket_url: str - the bucket url to store the translated documents\n    Returns: typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile]]:\n        translated_documents - translated raw txt documents as FlyteFiles\n        updated_metadatas - updated metadata as FlyteFiles\n    \"\"\"\n\n    # Iterate the documents and clean each\n    cleaned_documents = []\n    updated_metadatas = []\n    for i, (document, meta_file) in enumerate(zip(documents, metadatas)):\n        # Convert the metadata flytefile to a dataclass\n        metadata: DataLakeMetadata = flyte_file_to_input(meta_file, DataLakeMetadata)\n        logger.info(f\"[cleaning] Cleaning document {i} with uuid {metadata.document_id} of {len(documents)}\")\n\n        with document.open(\"r\") as file:\n            text = str(file.read())\n\n        # Perform text cleaning, return the input if it fails\n        try:\n            cleaned_text = clean_document_text(text)\n        except Exception as e:\n            logger.error(\n                f\"[cleaning] Error cleaning text for document {metadata.document_id}: {e} - returning original text without cleaning\"\n            )\n            clean_documents.append(document)\n            break\n\n        # Store the result back in the datalake\n        text_file = raw_file_to_flyte(\n            cleaned_text,\n            \"cleaned.txt\",\n            file_prefix=f\"{file_prefix}/{metadata.document_id}\",\n            bucket_url=bucket_url,\n        )\n\n        cleaned_documents.append(text_file)\n\n        # Note: We don't update the metdata but could add something like cleaned: true\n\n    return cleaned_documents, updated_metadatas\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/get_documents_from_collector.py\nimport os\nimport typing\nfrom pathlib import Path\n\nimport flytekit\nfrom flytekit import task\nfrom flytekit.types.directory import FlyteDirectory\nfrom flytekit.types.file import FlyteFile\n\n\n@task\ndef get_documents_from_collector(\n    bucket_url: str, collectors_root: str, collector_run_name: str\n) -> typing.List[FlyteFile]:\n    \"\"\"Returns a list of documents that were uploaded to S3 by a collector.\"\"\"\n    print(\"Getting collected documents...\")\n    working_dir = flytekit.current_context().working_directory\n    local_dir = Path(os.path.join(working_dir, \"collected_files\"))\n    local_dir.mkdir(exist_ok=True)\n\n    collector_run_path = os.path.join(collectors_root, collector_run_name)\n\n    file_dir = FlyteDirectory(path=local_dir, remote_directory=os.path.join(bucket_url, collector_run_path))\n\n    file_list = FlyteDirectory.listdir(file_dir)\n    print(f\"Number of files found: {len(file_list)} ({round(len(file_list) / 2)} documents)\")\n\n    return file_list\n```\n\nlangdetect\n```py data-warehouse-workflows/main/collectors/tasks/langdetect/lingua_detection.py\nfrom logging import Logger\n\nfrom lingua import Language, LanguageDetector, LanguageDetectorBuilder\n\nAUTO_DETECT = \"auto\"\n\n\nclass LinguaLanguageDetector:\n    \"\"\"Lingua language detector for detecting languages\"\"\"\n\n    def __init__(self, logger: Logger):\n        self.detector: LanguageDetector = (\n            LanguageDetectorBuilder.from_all_languages_without(\n                Language.BASQUE,\n                Language.BELARUSIAN,\n                Language.ESPERANTO,\n                Language.GANDA,\n                Language.LATIN,\n                Language.MAORI,\n                Language.NYNORSK,\n                Language.SHONA,\n                Language.SOTHO,\n                Language.TSONGA,\n                Language.TSWANA,\n                Language.XHOSA,\n                Language.YORUBA,\n                Language.ZULU,\n            )\n            .with_preloaded_language_models()\n            .build()\n        )\n\n        self.logger = logger\n\n    def detect_language(self, text: str) -> tuple[str, float]:\n        \"\"\"Detect the language of the text\"\"\"\n        language = self.detector.detect_language_of(text)\n\n        confidence = 0.0\n        if language:\n            code = language.iso_code_639_1.name.lower()\n\n            confidence_values = self.detector.compute_language_confidence_values(text)\n            if len(confidence_values) > 0:\n                confidence = confidence_values[0].value\n        else:\n            self.logger.warning(\"Cannot detect language, using auto-detect\")\n            code = AUTO_DETECT\n\n        self.logger.debug(f\"Detected language: {language}, code: {confidence}\")\n\n        return code, confidence\n```\n\n\n```py data-warehouse-workflows/main/collectors/tasks/load_document_to_warehouse.py\nimport typing\n\nimport flytekit\nimport grpc\nfrom data_warehouse_service.v1 import service_pb2, service_pb2_grpc\nfrom flytekit import task\nfrom flytekit.types.file import FlyteFile\nfrom opentelemetry import trace\n\nfrom collectors.custom_logging import get_logger\nfrom collectors.datatypes.document_models import (\n    Attribute,\n    DataLakeMetadata,\n    DataWarehouseDocument,\n    UpdateDocumentTextRequest,\n)\nfrom collectors.metrics import update_metrics\nfrom collectors.utils import flyte_file_to_input, output_to_flyte_file, raw_file_to_flyte\n\nlogger = get_logger()\ntracer = trace.get_tracer(\"datawarehouse\")\n\n\n@task(pod_template_name=\"data-warehouse-pod-template\")\n@tracer.start_as_current_span(\"load_document_to_warehouse\")\ndef load_document_to_warehouse(\n    documents: typing.List[typing.Dict[str, FlyteFile | str]],\n    grpc_endpoint: str,\n    bucket_url: str,\n    collector_id: str,\n    collector_name: str,\n    aggregator_type: str,\n) -> typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile], str]:\n    \"\"\"Load collectors documents into the database.\"\"\"\n    logger.info(\"Storing documents...\")\n\n    channel = grpc.insecure_channel(grpc_endpoint)\n    stub = service_pb2_grpc.DataWarehouseServiceStub(channel)\n\n    docs = [\n        {\"document\": flyte_file_to_input(document[\"flyte_file\"], DataWarehouseDocument), \"crawled\": document[\"crawled\"]}\n        for document in documents\n    ]\n\n    # Contains the project, domain, and ID (\"name\")\n    execution_data = flytekit.current_context().execution_id\n\n    file_prefix = \"data-warehouse\"\n    saved_documents = []\n    saved_metadata = []\n    num_successful = 0\n    num_failed = 0\n    last_successful_iteration = \"\"\n    for document_data in docs:\n        document = document_data[\"document\"]\n        crawled = document_data[\"crawled\"]\n        try:\n            # Convert the document into a protobuf message\n            document.attributes.append(Attribute(key=\"data_lake_prefix\", value_text=file_prefix))\n            request = service_pb2.CreateDocumentRequest(**document.model_dump())\n            response = stub.CreateDocument(\n                request,\n                metadata=(\n                    (\n                        \"requester\",\n                        f\"flyte_workflow:{execution_data.project}/{execution_data.domain}/{execution_data.name}\",\n                    ),\n                ),\n            )\n\n            trace.get_current_span().set_attribute(\"document_id\", response.document_id)\n            trace.get_current_span().add_event(\"Adding document to data warehouse\")\n            logger.info(\n                f\"[load_document_to_warehouse] Successfully loaded document {response.document_id} to warehouse\"\n            )\n\n            language = next((attr for attr in document.attributes if attr.key == \"original_language\"), None)\n            if language is not None and language.value_text == \"en\":\n                update_text = UpdateDocumentTextRequest(\n                    document_id=response.document_id, english_text=document.original_text\n                )\n                update_text_request = service_pb2.UpdateDocumentTextRequest(**update_text.model_dump())\n                stub.UpdateDocumentText(\n                    request=update_text_request,\n                    metadata=(\n                        (\n                            \"requester\",\n                            f\"flyte_workflow:{execution_data.project}/{execution_data.domain}/{execution_data.name}\",\n                        ),\n                    ),\n                )\n\n            path = f\"{file_prefix}/{response.document_id}\"\n            raw_file = raw_file_to_flyte(\n                document.original_text,\n                \"original.txt\",\n                file_prefix=path,\n                bucket_url=bucket_url,\n            )\n            saved_documents.append(raw_file)\n            flyte_file = output_to_flyte_file(\n                DataLakeMetadata(document_id=response.document_id, attributes=document.attributes),\n                \"metadata.json\",\n                file_prefix=path,\n                bucket_url=bucket_url,\n            )\n            saved_metadata.append(flyte_file)\n\n            num_successful += 1\n            last_successful_iteration = crawled\n            # TODO: how can we maintain same metrics across different tasks without replacing them?\n            # dec_backlog(collector_id, collector_name, aggregator_type)\n\n        except grpc.RpcError as e:\n            aggregator_reference = \"\"\n            for attr in document.attributes:\n                if attr.key == \"aggregator_reference\":\n                    aggregator_reference = attr.value_text\n                    break\n\n            # we need to update the last_successful_iteration if the document already\n            # exists so that we don't get stuck on the document in future collector runs\n            # don't increment number of successful or failed\n            if e.code() == grpc.StatusCode.ALREADY_EXISTS:\n                logger.info(\n                    \"Duplicate document - ignoring\",\n                    extra={\n                        \"collector_id\": collector_id,\n                        \"collector_name\": collector_name,\n                        \"aggregator_type\": aggregator_type,\n                        \"aggregator_reference\": aggregator_reference,\n                    },\n                )\n                last_successful_iteration = crawled\n                continue\n\n            logger.info(\n                \"Error storing document - it will be ignored\",\n                extra={\n                    \"error\": str(e),\n                    \"alert\": \"OsintWorkflowFailure\",\n                    \"collector_id\": collector_id,\n                    \"collector_name\": collector_name,\n                    \"aggregator_type\": aggregator_type,\n                    \"aggregator_reference\": aggregator_reference,\n                },\n            )\n            num_failed += 1\n            continue\n\n    update_metrics(num_successful, num_failed, collector_id, collector_name, aggregator_type)\n\n    if num_failed > 0:\n        # We don't want to fail the entire workflow here so just log\n        logger.info(f\"{num_failed}/{num_failed + num_successful} documents failed to save in data warehouse\")\n    return saved_documents, saved_metadata, last_successful_iteration\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/load_translation_to_warehouse.py\nimport logging\nimport typing\n\nimport flytekit\nimport grpc\nfrom data_warehouse_service.v1 import service_pb2, service_pb2_grpc\nfrom flytekit import task\nfrom flytekit.types.file import FlyteFile\n\nfrom collectors.datatypes.document_models import (\n    DataLakeMetadata,\n)\nfrom collectors.metrics import update_metrics\nfrom collectors.utils import flyte_file_to_input\n\nlogger = logging.getLogger()\n\n\n@task(pod_template_name=\"data-warehouse-pod-template\")\ndef load_translation_to_warehouse(\n    translated_documents: typing.List[FlyteFile],\n    translated_metadatas: typing.List[FlyteFile],\n    grpc_endpoint: str,\n    collector_id: str,\n    collector_name: str,\n    aggregator_type: str,\n):\n    \"\"\"Load translated documents and attributes into the database.\"\"\"\n    channel = grpc.insecure_channel(grpc_endpoint)\n    stub = service_pb2_grpc.DataWarehouseServiceStub(channel)\n\n    # Contains the project, domain, and ID (\"name\")\n    execution_data = flytekit.current_context().execution_id\n\n    num_successful = 0\n    num_failed = 0\n    for document_file, metadata_file in zip(translated_documents, translated_metadatas):\n        with document_file.open(\"r\") as file:\n            text = file.read()\n        metadata = flyte_file_to_input(metadata_file, DataLakeMetadata)\n\n        logging.info(f\"[translation] Storing translated documents {metadata.document_id} in the data warehouse\")\n        try:\n            stub.UpdateDocumentText(\n                request=service_pb2.UpdateDocumentTextRequest(\n                    document_id=metadata.document_id,\n                    english_text=str(text),\n                ),\n                metadata=(\n                    (\n                        \"requester\",\n                        f\"flyte_workflow:{execution_data.project}/{execution_data.domain}/{execution_data.name}\",\n                    ),\n                ),\n            )\n            logger.info(f\"[translation] Successfully updated document text {metadata.document_id} in data warehouse\")\n\n            stub.UpdateAttributesByDocumentID(\n                request=service_pb2.UpdateAttributesByDocumentIDRequest(\n                    document_id=metadata.document_id,\n                    attributes=[attr.model_dump() for attr in metadata.attributes],\n                ),\n                metadata=(\n                    (\n                        \"requester\",\n                        f\"flyte_workflow:{execution_data.project}/{execution_data.domain}/{execution_data.name}\",\n                    ),\n                ),\n            )\n            logger.info(\n                f\"[translation] Successfully updated attributes for document {metadata.document_id} in data warehouse\"\n            )\n\n            num_successful += 1\n\n        except grpc.RpcError as e:\n            print(handle_error)\n            handle_error(\n                aggregator_type=aggregator_type,\n                collector_id=collector_id,\n                collector_name=collector_name,\n                metadata=metadata,\n                e=e,\n            )\n            num_failed += 1\n            continue\n\n    update_metrics(num_successful, num_failed, collector_id, collector_name, aggregator_type)\n\n    if num_failed > 0:\n        # We don't want to fail the entire workflow here so just log\n        logger.warning(f\"{num_failed}/{num_failed + num_successful} documents failed to save in data warehouse\")\n\n\ndef handle_error(\n    aggregator_type: str, collector_id: str, collector_name: str, metadata: DataLakeMetadata, e: grpc.RpcError\n):\n    aggregator_reference = \"\"\n    for attr in metadata.attributes:\n        if attr.key == \"aggregator_reference\":\n            aggregator_reference = attr.value_text\n            break\n\n    # we need to update the last_successful_iteration if the document already\n    # exists so that we don't get stuck on the document in future collector runs\n    # don't increment number of successful or failed\n    if e.code() == grpc.StatusCode.ALREADY_EXISTS:\n        logger.info(\n            \"Duplicate document - ignoring\",\n            extra={\n                \"collector_id\": collector_id,\n                \"collector_name\": collector_name,\n                \"aggregator_type\": aggregator_type,\n                \"aggregator_reference\": aggregator_reference,\n            },\n        )\n\n    logger.error(\n        \"Error storing translated document - it will be ignored\",\n        extra={\n            \"error\": str(e),\n            \"alert\": \"OsintWorkflowFailure\",\n            \"collector_id\": collector_id,\n            \"collector_name\": collector_name,\n            \"aggregator_type\": aggregator_type,\n            \"aggregator_reference\": aggregator_reference,\n        },\n    )\n```\n\nrss\n```py data-warehouse-workflows/main/collectors/tasks/rss/__init__.py\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/rss/extract_from_rss.py\nimport base64\nimport hashlib\nimport json\nimport time\nimport typing\nfrom datetime import datetime\nfrom http import HTTPStatus\n\nimport flytekit\nimport validators\nfrom flytekit import Secret, task\nfrom flytekit.exceptions import user\nfrom flytekit.types.file import FlyteFile\n\nfrom collectors.datatypes.document_models import RSSPost\nfrom collectors.metrics import push_api_quota, push_backlog\nfrom collectors.utils import (\n    JsonLog,\n    fetch_rss_posts,\n    get_scrapingbee_remaining_quota,\n    raw_file_to_flyte,\n    scrape_content,\n)\nfrom custom_exceptions.custom_exceptions import InvalidFeedError\n\n\ndef get_url_for_item(item) -> str:\n    \"\"\"\n    For a given rss item, tries to return a useable url.\n\n    Most feeds will populate the link element, but some (https://ecipe.org/feed/) will populate guid instead.\n    \"\"\"\n    if \"link\" in item:\n        if validators.url(item[\"link\"]):\n            return item[\"link\"]\n\n    if \"guid\" in item:\n        if validators.url(item[\"guid\"]):\n            return item[\"guid\"]\n\n    return None\n\n\n@task(\n    pod_template_name=\"data-warehouse-pod-template\",\n    secret_requests=[Secret(group=\"data-collectors-credentials\", key=\"scraping_bee_api_key\")],\n)\ndef extract_from_rss(\n    feed_url: str,\n    collector_name: str,\n    collector_id: str,\n    batch_size: int,\n    collectors_root: str,\n    collector_run_name: str,\n    bucket_url: str,\n    last_successful_iteration: str,\n) -> typing.Tuple[typing.List[FlyteFile], str]:\n    \"\"\"Collect posts from an rss feed and create initial files from them in s3 bucket.\"\"\"\n    print(\"Collecting from rss feed...\")\n\n    context = flytekit.current_context()\n    token = context.secrets.get(\"data-collectors-credentials\", \"scraping_bee_api_key\")\n\n    code, quota = get_scrapingbee_remaining_quota(token)\n    if code != HTTPStatus.OK:\n        print(\"Failed to get scrapingbee quota.\")\n    else:\n        push_api_quota(quota, \"rss\")\n\n    dt = datetime.strptime(last_successful_iteration, \"%Y-%m-%dT%H:%M:%SZ\")\n    time_struct = dt.timetuple()\n\n    try:\n        posts, total_results = fetch_rss_posts(feed_url, time_struct)\n    # we expect a key error if the url is somehow not an rss/atom feed or is missing items\n    except InvalidFeedError as e:\n        print(\n            JsonLog(\n                msg=\"unable to fetch rss posts\",\n                err=str(e),\n                alert=\"OsintWorkflowFailure\",\n                collector_id=collector_id,\n                collector_name=collector_name,\n                aggregator_type=\"rss\",\n                url=feed_url,\n            )\n        )\n        raise\n\n    push_backlog(total_results, collector_id, collector_name, \"rss\")\n    print(f\"Total number of posts: {total_results}\")\n\n    output_files = []\n    published_at = last_successful_iteration\n    file_prefix = f\"{collectors_root}/{collector_run_name}\"\n\n    for count, (key, post) in enumerate(posts.items()):\n        if count >= batch_size:\n            print(\"Batch size reached.\")\n            break\n\n        resolved_url = get_url_for_item(post)\n\n        if resolved_url is None:\n            print(\n                JsonLog(\n                    msg=\"No valid url found for rss post\",\n                    collector_id=collector_id,\n                    collector_name=collector_name,\n                    aggregator_type=\"rss\",\n                )\n            )\n            continue\n\n        post_id = base64.urlsafe_b64encode(hashlib.md5(resolved_url.encode()).digest()).decode(\"ascii\")\n        title = post[\"title\"]\n        published_at = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", key)\n        categories = []\n        if post.get(\"tags\"):\n            categories = [c[\"term\"] for c in post.get(\"tags\")]\n        # attempt to scrape the content\n        # if we encounter an error with scraping, we shoud still return this document's publisehdAt so that it is skipped on the next run\n        try:\n            resolved_url, text = scrape_content(resolved_url, token)\n        except user.FlyteRecoverableException as e:\n            print(\n                JsonLog(\n                    msg=\"Failed to scrape url.\",\n                    error=str(e),\n                    alert=\"OsintWorkflowFailure\",\n                    collector_id=collector_id,\n                    collector_name=collector_name,\n                    aggregator_type=\"rss\",\n                    url=resolved_url,\n                )\n            )\n            return output_files, published_at\n\n        # if no text is found for the post after scraping and cleaning, skip it as it cannot be added to the data warehouse if empty\n        if not text:\n            print(\n                JsonLog(\n                    msg=\"No text found for rss post after scraping and cleaning\",\n                    url=resolved_url,\n                    collector_id=collector_id,\n                    collector_name=collector_name,\n                    aggregator_type=\"rss\",\n                )\n            )\n            continue\n\n        rss_post = RSSPost(\n            title=title, link=resolved_url, categories=categories, text=text, publishedAt=published_at\n        ).__dict__\n\n        # store files\n        meta_file = raw_file_to_flyte(\n            json.dumps(rss_post),\n            f\"{post_id}.meta\",\n            file_prefix=file_prefix,\n            bucket_url=bucket_url,\n        )\n        output_files.append(meta_file)\n        text_file = raw_file_to_flyte(\n            text,\n            f\"{post_id}.txt\",\n            file_prefix=file_prefix,\n            bucket_url=bucket_url,\n        )\n        output_files.append(text_file)\n\n    return output_files, published_at\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/rss/transform_rss.py\nimport base64\nimport hashlib\nimport typing\nfrom urllib.parse import urlparse\n\nfrom flytekit import task\nfrom flytekit.types.file import FlyteFile\n\nfrom collectors.datatypes.document_models import Attribute, DataWarehouseDocument, RSSPost\nfrom collectors.mapping.mapping import convert_to_timestamp, get_country_from_domain\nfrom collectors.utils import flyte_file_to_input, output_to_flyte_file\n\n\n@task\ndef transform_rss(\n    posts: typing.List[FlyteFile],\n    bucket_url: str,\n    collector_run_name: str,\n    collector_id: str,\n) -> typing.List[typing.Dict[str, FlyteFile | str]]:\n    \"\"\"Transform collected documents from files in s3 bucket and prepare standard attributes.\"\"\"\n    print(\"Processing rss files into documents...\")\n\n    output_files: typing.List[typing.Dict[str, FlyteFile | str]] = []\n    file_prefix = f\"transform/{collector_run_name}\"\n\n    rssposts = [post for post in posts if post.path.endswith(\".meta\")]\n\n    for rsspost in rssposts:\n        rss = flyte_file_to_input(rsspost, RSSPost)\n        # Should only be None for tests. Set it to \"\" in this case to not fail the unit tests.\n        remote_source = \"\" if rsspost.remote_source is None else rsspost.remote_source\n\n        post_id = base64.urlsafe_b64encode(hashlib.md5(rss.link.encode()).digest()).decode(\"ascii\")\n        published_date_timestamp = convert_to_timestamp(rss.publishedAt)\n\n        link = rss.link\n        if \"//\" not in link:\n            # Add a // to the link so urlparse can parse it correctly\n            link = \"//\" + link\n\n        # rss.link is a full URL, so we use urlparse to get the domain\n        source_domain = urlparse(link).hostname\n\n        attributes = [\n            Attribute(key=\"publisher\", value_text=\"rss\"),\n            Attribute(key=\"aggregator_reference\", value_text=f\"RSS:{post_id}\"),\n            Attribute(key=\"title\", value_text=rss.title or f\"Untitled document from {source_domain}\"),\n            Attribute(key=\"source_domain\", value_text=source_domain),\n            Attribute(key=\"published_date\", value_timestamp=published_date_timestamp),\n            Attribute(key=\"collected_file_url\", value_text=remote_source),\n            Attribute(key=\"collector_ids\", value_text_array=[collector_id]),\n            Attribute(key=\"original_location\", value_text=rss.link),\n        ]\n\n        if len(rss.categories) > 0:\n            attributes.append(Attribute(key=\"topic\", value_text_array=rss.categories))\n\n        geographic_domain = get_country_from_domain(rss.link)\n        if geographic_domain is not None:\n            attributes.append(Attribute(key=\"geographic_domain\", value_text=geographic_domain))\n\n        document = DataWarehouseDocument(source_aggregator=\"RSS\", original_text=rss.text, attributes=attributes)\n        flyte_file = output_to_flyte_file(\n            document,\n            f\"{post_id}.json\",\n            file_prefix=file_prefix,\n            bucket_url=bucket_url,\n        )\n        output_files.append({\"crawled\": rss.publishedAt, \"flyte_file\": flyte_file})\n\n    return output_files\n```\n\n\n```py data-warehouse-workflows/main/collectors/tasks/send_batch_to_inference.py\nimport typing\n\nfrom flytekit import task\nfrom flytekit.types.file import FlyteFile\n\n\n@task\ndef send_batch_to_inference(documents: typing.List[FlyteFile], shared_bucket_url: str) -> typing.List[FlyteFile]:\n    \"\"\"Send a batch of documents to the inference service.\"\"\"\n\n    print(\"Sending documents to inference service...\")\n\n    return []\n```\n\ntext_cleaning\n```py data-warehouse-workflows/main/collectors/tasks/text_cleaning/cleaning.py\nimport logging\nimport os\nimport re\n\nfrom bs4 import BeautifulSoup\nfrom markdown import markdown\n\nfrom collectors.tasks.text_cleaning.sentencizer import get_sentences\n\nlogger = logging.getLogger()\n\nCLEAN_THRESHOLD = float(os.getenv(\"CLEAN_THRESHOLD\", \"0.65\"))\nMIN_CHAR_PER_SENTENCE = int(os.getenv(\"MIN_CHAR_PER_SENTENCE\", \"4\"))\n\n# Compiled regex patterns\nMARKDOWN_LINK_PATTERN = re.compile(r\"\\[([^\\]]+)\\]\\(.*?\\)\")\nXML_TAG_PATTERN = re.compile(r\"<[^>]*>\")\nXML_DECL_PATTERN = re.compile(r\"<?xml.*?>\")\nURL_PATTERN = re.compile(\n    r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?]))\"  # noqa RUF001\n)\nTRANSLATION_PROVIDER_PATTERN = re.compile(r\"Provider Name:.*\")\nLOCATION_PATTERN = re.compile(r\"Original Location:.*\")\nTITLE_PATTERN = re.compile(r\"Title:.*\")\nJAVASCRIPT_PATTERN = re.compile(r\"\\[[^\\]]*?\\]\\(javascript: ?void ?\\(0\\)*;*\\)?\", flags=re.IGNORECASE)\nCITATION_PATTERN = re.compile(r\"[\\[\\(](\\d;? ?)+[\\]\\)]\")\nQUESTION_MARK_PATTERN = re.compile(r\"\\?+\")\nSPACE_PATTERN = re.compile(r\" +\")\nEMOJI_PATTERN = re.compile(\n    r\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001F200-\\U0001F2FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001F000-\\U0001F02F\\U0001F030-\\U0001F09F\\U0001F0A0-\\U0001F0FF\\U0001F100-\\U0001F1FF\\U0001F200-\\U0001F2FF\\U0001F300-\\U0001F5FF\\\"]\"\n)\nREPEATED_TEXT_PATTERN = re.compile(r\"(\\w{3,})\\1{3,}\")\nLEFT_BRACKET_PATTERN = re.compile(r\".*\\>\")\nRIGHT_BRACKET_PATTERN = re.compile(r\"\\<.*\")\nHTML_TAG_PATTERN = re.compile(r\"<.*?>\")\nDATE_PATTERN = re.compile(r\"[\\s\\(\\[\\{]\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}[\\s\\)\\]\\}]\")\nTIME_PATTERN = re.compile(r\"[\\s\\(\\[\\{]\\d{1,2}:\\d{1,2}[\\s\\)\\]\\}]\")\nRANDOM_STRING_PATTERN = re.compile(r\"[\\s\\(\\[\\{]\\w{1,2}[\\s\\)\\]\\}]\")\nPARENTHESIS_PATTERN = re.compile(r\"\\(.*?\\)\")\nPUNCTUATION_PATTERN = re.compile(r\" +([.,!?])\")\n# Explanation:\n# # [ ]`: Denotes a character class\n# ^`: Negation symbol, matches any character that is not in the specified set.\n# `\\w`: Matches any word character (letter, digit, or underscore).\n# `\\s`: Matches any whitespace character (space, tab, newline, etc.).\n# '|_': Matches underscore\nSYMBOL_PATTERN = re.compile(r\"[^\\w\\s]|_\")\n\n\ndef clean_document_text(document_text: str) -> str:\n    \"\"\"Performs various text cleaning\n\n    :param document_text: The text to clean\n    :return: The cleaned text\"\"\"\n    # clean entire text\n    logger.info(\"Beginning quick cleaning\")\n    quick_cleaned_text = clean_text(document_text)\n\n    # clean individual sentences\n    sentences = get_sentences(quick_cleaned_text)\n    cleaned_sentences = []\n    for sentence in sentences:\n        cleaned_sentence = clean_text(sentence)\n        cleaned_sentence = remove_text_left_to_brackets(cleaned_sentence)\n        cleaned_sentence = remove_text_right_to_brackets(cleaned_sentence)\n        if cleaned_sentence:\n            cleaned_sentences.append(cleaned_sentence)\n\n    # remove duplicate sentences\n    cleaned_sentences = list(dict.fromkeys(cleaned_sentences))\n    # get valid sentences\n    valid_sentences = [sent for sent in cleaned_sentences if is_valid_sentence(sent)]\n\n    # reassemble cleaned sentences\n    document_text = \" \".join(valid_sentences)\n    return document_text\n\n\ndef remove_markdown_text_with_links(text: str) -> str:\n    # Remove markdown text with links including the text in the brackets\n    return MARKDOWN_LINK_PATTERN.sub(r\"\\1\", text)\n\n\n# returns a tuple of the cleaned text and a list of links\ndef remove_markdown(text: str) -> tuple[str, list[str]]:\n    html = markdown(text)\n    bs = BeautifulSoup(html, features=\"html.parser\")\n    links = [i.get(\"href\") for i in bs.findAll(\"a\")]\n    if links:\n        texts = [i.string if i.string is not None else \"\" for i in bs.findAll(\"a\")]\n        results = zip(texts, links, strict=False)\n        rep = {re.escape(f\"[{k}]({v})\"): k for (k, v) in results}\n        pat = re.compile(\"|\".join(rep.keys()))\n        text = pat.sub(lambda m: rep[re.escape(m.group(0))], text)\n    return text, [link for link in links if link != \"\"]\n\n\ndef remove_xml(text: str) -> str:\n    # Remove XML tags\n    text = XML_TAG_PATTERN.sub(\"\", text)\n    # Remove XML declarations\n    text = XML_DECL_PATTERN.sub(\"\", text)\n    return text\n\n\ndef remove_urls(text: str) -> tuple[str, list[str]]:\n    urls = [\"\".join(tup) for tup in URL_PATTERN.findall(text)]\n    return URL_PATTERN.sub(\"\", text), urls\n\n\ndef remove_metadata(text: str) -> str:\n    # Remove the metadata from the text\n    text = TRANSLATION_PROVIDER_PATTERN.sub(\"\", text)\n    text = LOCATION_PATTERN.sub(\"\", text)\n    text = TITLE_PATTERN.sub(\"\", text)\n    return text\n\n\ndef remove_javascript(text: str) -> str:\n    return JAVASCRIPT_PATTERN.sub(\"\", text)\n\n\ndef remove_citations(text: str) -> str:\n    return CITATION_PATTERN.sub(\"\", text)\n\n\ndef reduce_question_marks(text: str) -> str:\n    return QUESTION_MARK_PATTERN.sub(\"?\", text)\n\n\ndef reduce_spaces(text: str) -> str:\n    return SPACE_PATTERN.sub(\" \", text).strip()\n\n\ndef remove_emojis(text: str) -> str:\n    return EMOJI_PATTERN.sub(\"\", text)\n\n\ndef remove_too_much_repeated_text(text: str) -> str:\n    return REPEATED_TEXT_PATTERN.sub(\"\", text)\n\n\ndef remove_text_left_to_brackets(text: str) -> str:\n    try:\n        text = LEFT_BRACKET_PATTERN.sub(\"\", text)\n        return text\n    # except when None\n    except Exception as e:\n        logger.debug(f\"Error: {e}\")\n        return None\n\n\ndef remove_text_right_to_brackets(text: str) -> str:\n    try:\n        text = RIGHT_BRACKET_PATTERN.sub(\"\", text)\n        return text\n    except Exception as e:\n        logger.debug(f\"Error: {e}\")\n        # return None\n\n\ndef remove_all_html_tags(text: str) -> str:\n    return HTML_TAG_PATTERN.sub(\"\", text)\n\n\ndef remove_dates_and_surrounding_symbols(text: str) -> str:\n    return DATE_PATTERN.sub(\"\", text)\n\n\ndef remove_times_and_surrounding_symbols(text: str) -> str:\n    return TIME_PATTERN.sub(\"\", text)\n\n\ndef remove_random_string_and_symbol_combinations(text: str) -> str:\n    return RANDOM_STRING_PATTERN.sub(\"\", text)\n\n\ndef remove_string_inside_parentheses(text: str) -> str:\n    return PARENTHESIS_PATTERN.sub(\"\", text)\n\n\ndef remove_space_before_punctuation(text: str) -> str:\n    return PUNCTUATION_PATTERN.sub(r\"\\1\", text)\n\n\ndef remove_hypen_space_pattern(text: str) -> str:\n    return text.replace(\"- \", \"\")\n\n\ndef clean_text(text: str) -> str:\n    intermediate = remove_too_much_repeated_text(text)\n    intermediate = remove_javascript(intermediate)\n    intermediate, _ = remove_markdown(intermediate)\n    intermediate, _ = remove_urls(intermediate)\n    intermediate = remove_citations(intermediate)\n    intermediate = remove_emojis(intermediate)\n    intermediate = remove_all_html_tags(intermediate)\n    intermediate = remove_xml(intermediate)\n    intermediate = reduce_question_marks(intermediate)\n    intermediate = remove_dates_and_surrounding_symbols(intermediate)\n    intermediate = remove_times_and_surrounding_symbols(intermediate)\n    intermediate = remove_string_inside_parentheses(intermediate)\n    intermediate = remove_space_before_punctuation(intermediate)\n    cleaned_text = reduce_spaces(intermediate)\n\n    cleaned_text = cleaned_text.strip()\n    if not cleaned_text:\n        return None\n    if cleaned_text[-1] not in [\".\", \"?\", \"!\"]:\n        cleaned_text += \".\"\n    return cleaned_text\n\n\ndef word_contains_symbols(word: str) -> bool:\n    return bool(SYMBOL_PATTERN.search(word))\n\n\ndef is_valid_sentence(sentence: str) -> bool:\n    # calculate ratios for alphanumeric chars in sentence\n    total_count = len(sentence)\n    alnum_count = sum(c.isalnum() for c in sentence)\n\n    # calculate ratios for words in sentence\n    words = sentence.split()\n    num_words = len(words)\n    symbol_words = sum(word_contains_symbols(w) for w in words)\n\n    # conditions for validating sentence\n    # cond_1: ratio of alphanumeric chars to sentence length is larger than threshold\n    # and ratio of alphanumeric chars to sentence length is larger than ratio of words with symbols\n    cond_1 = alnum_count / total_count >= CLEAN_THRESHOLD\n    # cond_2: alphanumeric chars to sentence length is larger than ratio of words with symbols to total number of words\n    cond_2 = alnum_count / total_count >= symbol_words / num_words\n    # cond_3: sentence length is larger than min_char_per_sentence\n    cond_3 = total_count > MIN_CHAR_PER_SENTENCE\n\n    is_valid = cond_1 and cond_2 and cond_3\n    return is_valid\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/text_cleaning/sentencizer.py\nimport pysbd\n\nsentencizer = pysbd.Segmenter(language=\"en\", clean=False)\n\n\ndef get_sentences(text: str, strip: bool = True) -> tuple[list[str], list[str]]:\n    sentences = sentencizer.segment(text)\n    if strip:\n        sentences = [sentence.strip() for sentence in sentences]\n    return sentences\n```\n\n\n```py data-warehouse-workflows/main/collectors/tasks/transform_documents.py\nimport typing\n\nfrom flytekit import task\nfrom flytekit.types.file import FlyteFile\n\nfrom collectors.tasks.rss.transform_rss import transform_rss\nfrom collectors.tasks.webz.transform_webz import transform_webz\n\n\n@task\ndef transform_documents(\n    posts: typing.List[FlyteFile],\n    bucket_url: str,\n    collector_run_name: str,\n    collector_id: str,\n    aggregator_type: str,\n) -> typing.List[typing.Dict[str, FlyteFile | str]]:\n    \"\"\"Transform collected documents from files in s3 bucket and prepare standard attributes.\"\"\"\n\n    if aggregator_type == \"webz\":\n        return transform_webz(\n            posts=posts,\n            bucket_url=bucket_url,\n            collector_run_name=collector_run_name,\n            collector_id=collector_id,\n        )\n    elif aggregator_type == \"rss\":\n        return transform_rss(\n            posts=posts,\n            bucket_url=bucket_url,\n            collector_run_name=collector_run_name,\n            collector_id=collector_id,\n        )\n    else:\n        raise ValueError(f\"Unsupported aggregator type: {aggregator_type}\")\n```\n\ntranslate\n```py data-warehouse-workflows/main/collectors/tasks/translate/chunking.py\nimport io\nimport re\nfrom collections.abc import Generator\n\nfrom pydantic import BaseModel, ConfigDict\n\nreg_exp_for_breaking_chunks = re.compile(r\"[^\\n.?!]+$\")\n\n\nclass Chunk(BaseModel):\n    data: str\n    size: int\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n\ndef find_breaking_point(text: str) -> int:\n    \"\"\"Find the point at which to break the text.\"\"\"\n    max_loc_index = len_without_split_utf8_char(text.encode())\n\n    # Avoid returning zero length if the string is totally invalid\n    if max_loc_index == 0:\n        max_loc_index = len(text)\n\n    text_str = text[:max_loc_index]\n    match = reg_exp_for_breaking_chunks.search(text_str)\n    if match:\n        # Convert string index back to bytes index\n        loc = len(text_str[: match.start()])\n        if loc >= 1:\n            return loc\n\n    return max_loc_index\n\n\ndef len_without_split_utf8_char(text: bytes) -> int:\n    \"\"\"Returns the length without splitting a UTF-8 character in half\"\"\"\n    length = len(text)\n    while length > 0:\n        last_char_len = get_last_utf8_char_len(text[:length])\n        if last_char_len > 0:\n            break\n        length -= 1\n    return length\n\n\ndef get_last_utf8_char_len(text: bytes) -> int:\n    \"\"\"\n    Determines the length of the last UTF-8 character in a bytes object.\n\n    Args:\n        text (bytes): The bytes object to inspect.\n\n    Returns:\n        int: The length (in bytes) of the last UTF-8 character, or 0 if none is found.\n    \"\"\"\n    max_len = min(4, len(text))  # Calculate the max UTF-8 char length (up to 4 bytes) or length of text\n    for i in range(1, max_len + 1):\n        try:\n            text[-i:].decode(\"utf-8\")  # Try to decode the last 'i' bytes as UTF-8\n            return i  # Decoding successful, return length of last UTF-8 character\n        except UnicodeDecodeError:\n            continue  # Decoding failed, try the next possible length\n    return 0  # No valid UTF-8 character found, return 0\n\n\ndef chunk_file(\n    file_size: int,\n    stream: io.IOBase,\n    max_chunk_size: int,\n) -> Generator[Chunk, None, None]:  # Generator[yield_type, send_type, return_type]\n    \"\"\"\n    chunk_file streams chunks. If the file is small enough it returns a single chunk.\n    Otherwise it returns multiple chunks.\n\n    Args:\n        file_size (int): The file size of the document used to interate over each byte of the document.\n        stream (io.IOBase): A Byte stream of the document used to read a chunks worht of data.\n        max_chunk_size (int): Maximum allowed size of chunk\n    \"\"\"\n\n    #  If we only need a single chunk, just return a single chunk\n    if file_size < max_chunk_size:\n        yield Chunk(data=stream.read(file_size), size=file_size)\n        return\n\n    buffer = \"\"\n    chars_remaining_to_read = file_size\n    num_chunks = 0\n\n    try:\n        while chars_remaining_to_read > 0:\n            chars_to_read_this_time = min(max_chunk_size - len(buffer), chars_remaining_to_read)\n            newly_read = stream.read(chars_to_read_this_time)\n            chars_remaining_to_read -= len(newly_read)\n\n            if chars_remaining_to_read == 0:\n                chars_to_write = buffer + newly_read\n                yield Chunk(data=chars_to_write, size=len(chars_to_write))\n            else:\n                loc = find_breaking_point(newly_read)\n                bytes_to_write = buffer + newly_read[:loc]\n                buffer = newly_read[loc:]\n                yield Chunk(data=bytes_to_write, size=len(bytes_to_write))\n\n            num_chunks += 1\n\n    finally:\n        stream.close()\n\n    return\n```\n\nproviders\n```py data-warehouse-workflows/main/collectors/tasks/translate/providers/llm_provider.py\nimport io\nimport logging\nimport os\nfrom collections.abc import Generator\nfrom logging import Logger\n\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nfrom collectors.config import Config\nfrom collectors.tasks.translate.chunking import Chunk, chunk_file\nfrom collectors.tasks.translate.providers.provider import TranslationProvider\nfrom collectors.utils import make_local_dir\n\nfile_prefix = \"data-warehouse\"\n\nlogger = logging.getLogger()\n\n\nclass LlmTranslationProviderError(Exception):\n    pass\n\n\nclass LlmTranslationProvider(TranslationProvider):\n    def __init__(\n        self,\n        config,\n        logger,\n    ):\n        super().__init__(config, logger)\n        self.client: instructor.Instructor = instructor.from_openai(\n            OpenAI(\n                base_url=config.OPENAI_BASE_URL,\n                api_key=config.OPENAI_API_KEY,\n                max_retries=3,\n                timeout=60,\n            )\n        )\n        self.config: Config = config\n        self.logger: Logger = logger\n\n    def translate(\n        self,\n        source_lang: str,\n        title: str,\n        text: str,\n        document_id: str,\n    ) -> tuple[str, str, int]:\n        \"\"\"\n        Args:\n\n        Returns: tuple[str, str, int]:\n            translated_local_file,\n            translated_title,\n            translated_file_size\n        \"\"\"\n        total_token_count = 0\n\n        logger.debug(f\"[translation] Translating '{title:10}...' title to {source_lang}\")\n        translated_title, tokens_used = self.translate_title(source_lang, title)\n        total_token_count += tokens_used\n\n        # Determine the char_size of the file for use in chunking\n        char_size = len(text)\n\n        # Create a file to write the translated data to\n        translated_local_file = make_local_dir(f\"{file_prefix}/{document_id}\", \"translated.txt\")\n\n        with open(translated_local_file, \"w\") as local_file:\n            # Chunk the file into parts - count the chunks manually as chunks is a generator - if there is one chunk translate the whole text\n            chunk_count = 0\n            stream = io.StringIO(text)\n            chunks: Generator[Chunk] = chunk_file(\n                file_size=char_size, stream=stream, max_chunk_size=self.config.TRANSLATION_MAX_LLM_CHAR_SIZE\n            )\n\n            for chunk in chunks:\n                # 2.1. Translate each chunk - translated title will be the same for each so only send once\n                translated_chunk, tokens_used = self.translate_chunk(source_lang, chunk.data)\n                total_token_count += tokens_used\n\n                # 2.2. Add the file to a local variable\n                local_file.write(translated_chunk)\n\n                logger.info(\n                    \"[translation] chunking file for translation service\",\n                    extra={\n                        \"uploadId\": document_id,\n                        \"chunkNum\": chunk_count,\n                        \"charSize\": chunk.size,\n                        \"maxChunkSize\": self.config.TRANSLATION_MAX_LLM_CHAR_SIZE,\n                    },\n                )\n                chunk_count += 1\n        translated_file_size = os.path.getsize(translated_local_file)\n\n        logger.info(\n            \"[translation] Successfully finished LLM translation\",\n            extra={\n                \"document_id\": document_id,\n                \"source_lang\": source_lang,\n                \"model\": self.config.OPENAI_MODEL,\n                \"conf_should_chunk_files\": self.config.LLM_CHUNK_FILES,\n                \"file_char_size\": char_size,\n                \"tokens_used\": total_token_count,\n            },\n        )\n        return translated_local_file, translated_title, translated_file_size\n\n    def translate_title(self, source_language: str, title: str) -> tuple[str, int]:\n        prompt = f\"Translate the following title from {source_language} to {self.config.TARGET_LANGUAGE} (note: if nothing needs to be translated, just return the original title):\\n\\n{title}\"\n        return self.perform_translation(prompt)\n\n    def translate_chunk(self, source_language: str, text: str) -> tuple[str, int]:\n        prompt = f\"Translate the following text from {source_language} to {self.config.TARGET_LANGUAGE}:\\n\\n{text}\"\n        return self.perform_translation(prompt)\n\n    def perform_translation(self, prompt: str) -> tuple[str, int]:\n        try:\n\n            class TranslationResponse(BaseModel):\n                text: str = Field(description=\"Translation response text\")\n\n            result, response = self.client.chat.completions.create_with_completion(\n                model=self.config.OPENAI_MODEL,\n                response_model=TranslationResponse,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a translator. Focus on the main news and leave out advertisements and other news\",\n                    },\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            )\n\n            return result.text, response.usage.total_tokens\n\n        except LlmTranslationProviderError as e:\n            self.logger.error(f\"[translation] Error in translate_chunk: {e!s}\")\n            raise\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/translate/providers/provider.py\n\"\"\"An abstract for all lang providers\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom logging import Logger\n\nfrom flytekit.types.file import FlyteFile\nfrom pydantic import BaseModel\n\nfrom collectors.config import Config\n\n\n# Add datatypes for the translation providers\nclass TranslationResult(BaseModel):\n    new_file: FlyteFile\n    new_title: str\n    new_file_size: int\n\n\nclass TranslationProvider(ABC):\n    def __init__(\n        self,\n        config: Config,\n        logger: Logger,\n    ):\n        self.config = config\n        self.logger = logger\n\n    @abstractmethod\n    def translate(\n        self,\n        source_lang: str,\n        title: str,\n        text: str,\n        document_id: str,\n    ) -> tuple[str, str, int]:\n        \"\"\"Abstract translation method\n\n        Args:\n            source_lang: The language of the text to be translated\n            title: The title of the text to be translated\n\n            char_size: The size of the text to be translated\n            document_id: The id of the upload\n        \"\"\"\n        pass\n```\n\n\n\n```py data-warehouse-workflows/main/collectors/tasks/translate_documents.py\nimport logging\nimport typing\nfrom datetime import timedelta\n\nfrom flytekit import FlyteFile, Resources, task\n\nfrom collectors.config import get_config\nfrom collectors.datatypes.document_models import Attribute, DataLakeMetadata, find_attribute\nfrom collectors.tasks.langdetect.lingua_detection import LinguaLanguageDetector\nfrom collectors.tasks.translate.providers.llm_provider import LlmTranslationProvider, LlmTranslationProviderError\nfrom collectors.utils import flyte_file_to_input, output_to_flyte_file\n\nconfig = get_config()\nlogger = logging.getLogger()\n\nfile_prefix = \"data-warehouse\"\ntarget_lang = \"en\"\n# Init these at the module scope and then poppulate when required\ntranslation_provider = None\nlanguage_detection_provider = None\n\n\n@task(\n    pod_template_name=\"data-warehouse-pod-template\",\n    interruptible=True,\n    retries=3,\n    timeout=timedelta(minutes=5),\n    requests=Resources(cpu=\"0.4\", mem=\"600Mi\"),\n    limits=Resources(mem=\"2000Mi\"),\n)\ndef translate_documents(  # noqa: C901, PLR0912\n    documents: typing.List[FlyteFile], metadatas: typing.List[FlyteFile], bucket_url: str\n) -> typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile]]:\n    \"\"\"\n    Takes a list of raw documents FlyteFiles and a list of DataLakeMetadata metadata.json files and if not\n    already the targer language `en`, translates to the target language.\n\n    For WEBZ we trust the DataLakeMetadata.Attribute('original_language') property\n    For RSS we run language detection using the LinguaLanguageDetector\n\n    - Updates the 'translated' in the DataLakeMetadata metadata.json file to 'true' if translated\n    - Updates the 'title' in the DataLakeMetadata metadata.json file to the translated title if translated\n    - Returns flytefiles which will be uploaded to their defined location in the datalake\n\n    Args:\n        documents: typing.List[FlyteFile] - typing.List of the document contents\n        metadatas: typing.List[FlyteFile] - typing.List of the document metadata\n        bucket_url: str - the bucket url to store the translated documents\n        aggregator_type: str - the type of aggregator that the documents came from i.e {webz, rss}\n    Returns: typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile]]:\n        translated_documents - translated raw txt documents as FlyteFiles\n        updated_metadatas - updated metadata as FlyteFiles\n    \"\"\"\n\n    if not config.FEATURE_TRANSLATION_ENABLED:\n        logger.info(\"[translation] Feature flag is currently false, skipping...\")\n        return [], []\n\n    translated_documents = []\n    updated_metadatas = []\n    for i, (document, meta_file) in enumerate(zip(documents, metadatas)):\n        logger.debug(f\"[translation] Beginning translation for document {i} of {len(documents)}\")\n        # Convert the metadata flytefile to a dataclass\n        metadata: DataLakeMetadata = flyte_file_to_input(meta_file, DataLakeMetadata)\n\n        # Download the data so that\n        with document.open(\"r\") as file:\n            text = str(file.read())\n\n        # Determine the language\n        attr = find_attribute(metadata.attributes, \"original_language\")\n        if not attr:\n            logger.info(\n                f\"[translation] Document {metadata.document_id} has no original language, will try language detection\"\n            )\n\n            # The lib is mem intensive so only init if needed\n            language_detection_provider = init_lang_detect(config, logger)\n\n            (original_language, confidence) = language_detection_provider.detect_language(text)\n            logger.debug(f\"[translation] Detected language {original_language} with confidence {confidence}\")\n            if confidence < config.LANGUAGE_DETECTION_THRESHOLD:\n                logger.warning(f\"[translation] Document {metadata.document_id} has low confidence, will not translate\")\n                continue\n        else:\n            original_language = attr.value_text\n\n        # Do nothing if the original_language is the target lang\n        if original_language == target_lang:\n            logger.info(\n                f\"[translation] Document with uid {metadata.document_id} is already target language `{target_lang}, skipping translation`\"\n            )\n            continue\n\n        # Get the title of the RSS or Webz document\n        attr = find_attribute(metadata.attributes, \"title\")\n        if not attr:\n            logger.warning(f\"[translation] Document {metadata.document_id} has no title, will use default\")\n        title = attr.value_text if attr else \"Unititled Document\"\n\n        try:\n            translation_provder = init_translation_provider(config, logger)\n            # The translate function will take the input file key\n            translated_document_path, translated_title, _ = translation_provder.translate(\n                source_lang=original_language,\n                title=title,\n                text=text,\n                document_id=metadata.document_id,\n            )\n        except LlmTranslationProviderError as e:\n            logger.error(f\"[translation] Error translating document {metadata.document_id}: {e}\")\n            raise\n\n        translated_documents.append(\n            FlyteFile(\n                path=translated_document_path,\n                remote_path=f\"{bucket_url}/{file_prefix}/{metadata.document_id}/translated.txt\",\n            )\n        )\n\n        # Update the metadata in original\n        updated_attributes = []\n        for attr in metadata.attributes:\n            if attr.key == \"title\":\n                attr.value_text = translated_title\n                updated_attributes.append(attr)\n            elif attr.key == \"translated\":\n                attr.value_text = \"true\"\n                updated_attributes.append(attr)\n            else:\n                updated_attributes.append(attr)\n\n        if all(\"translated\" != attr.key for attr in metadata.attributes):\n            updated_attributes.append(Attribute(key=\"translated\", value_text=\"true\"))\n\n        metadata.attributes = updated_attributes\n        updated_metadatas.append(\n            output_to_flyte_file(\n                metadata,\n                \"metadata.json\",\n                file_prefix=f\"{file_prefix}/{metadata.document_id}\",\n                bucket_url=bucket_url,\n            )\n        )\n\n    return translated_documents, updated_metadatas\n\n\ndef init_translation_provider(config, logger):\n    \"\"\"\n    Init the var once so that we init when needed and only once\n    \"\"\"\n    global translation_provider  # noqa: PLW0603\n    if translation_provider is not None:\n        return translation_provider\n    translation_provider = LlmTranslationProvider(config=config, logger=logger)\n    return translation_provider\n\n\ndef init_lang_detect(config, logger):\n    \"\"\"\n    Init the var once so that we init when needed and only once\n    \"\"\"\n    global language_detection_provider  # noqa: PLW0603\n    if language_detection_provider is not None:\n        return language_detection_provider\n    language_detection_provider = LinguaLanguageDetector(logger=logger)\n    return language_detection_provider\n```\n\nwebz\n```py data-warehouse-workflows/main/collectors/tasks/webz/__init__.py\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/webz/extract_from_webz.py\nimport json\nimport time\nimport typing\n\nimport flytekit\nfrom flytekit import Secret, task\nfrom flytekit.types.file import FlyteFile\n\nfrom collectors.metrics import push_api_quota, push_backlog\nfrom collectors.utils import JsonLog, clean_text, fetch_posts, raw_file_to_flyte\n\n\n@task(\n    pod_template_name=\"data-warehouse-pod-template\",\n    secret_requests=[Secret(group=\"data-collectors-credentials\", key=\"WebzToken\")],\n)\ndef extract_from_webz(\n    query: str,\n    collector_name: str,\n    collector_id: str,\n    params: str,\n    batch_count: int,\n    collectors_root: str,\n    collector_run_name: str,\n    bucket_url: str,\n) -> typing.List[FlyteFile]:\n    \"\"\"Collect posts from webz based on a query and create initial files from them in s3 bucket.\"\"\"\n    print(\"Collecting from webz...\")\n\n    context = flytekit.current_context()\n    token = context.secrets.get(\"data-collectors-credentials\", \"WebzToken\")\n\n    params_dict = json.loads(params)\n    try:\n        posts, total_results, requests_left = fetch_posts(query, params_dict, batch_count, token)\n    # catch any exceptions that occur to make sure we redact the webz token in error messages\n    except Exception as e:\n        error_message = str(e)\n        redacted_message = error_message.replace(token, \"[REDACTED]\")\n        print(\n            JsonLog(\n                msg=\"Failed to fetch posts.\",\n                error=redacted_message,\n                alert=\"OsintWorkflowFailure\",\n                collector_id=collector_id,\n                collector_name=collector_name,\n                aggregator_type=\"webz\",\n            )\n        )\n        raise Exception(\"Failed to fetch posts.\") from None\n\n    # set the api quota for webz and backlog for the collector\n    push_api_quota(requests_left, \"webz\")\n    push_backlog(total_results, collector_id, collector_name, \"webz\")\n\n    print(f\"Number of posts fetched out of {total_results}: {len(posts)}\")\n\n    output_files = []\n    file_prefix = f\"{collectors_root}/{collector_run_name}\"\n\n    if posts is not None:\n        for post in posts:\n            # extend post information\n            post[\"queryName\"] = collector_name\n            post[\"query\"] = query\n            post[\"fetchTime\"] = time.time()\n\n            post[\"text\"] = clean_text(post[\"text\"])\n\n            # if no text is found for the post after cleaning, skip it as it cannot be added to the data warehouse if empty\n            if not post[\"text\"]:\n                print(\n                    JsonLog(\n                        msg=\"No text found for webz post after cleaning\",\n                        url=post[\"thread\"][\"url\"],\n                        collector_id=collector_id,\n                        collector_name=collector_name,\n                        aggregator_type=\"webz\",\n                    )\n                )\n                continue\n\n            # store files\n            meta_file = raw_file_to_flyte(\n                json.dumps(post),\n                f\"{post['thread']['uuid']}.meta\",\n                file_prefix=file_prefix,\n                bucket_url=bucket_url,\n            )\n            output_files.append(meta_file)\n            text_file = raw_file_to_flyte(\n                post[\"text\"],\n                f\"{post['thread']['uuid']}.txt\",\n                file_prefix=file_prefix,\n                bucket_url=bucket_url,\n            )\n            output_files.append(text_file)\n\n    return output_files\n```\n\n```py data-warehouse-workflows/main/collectors/tasks/webz/transform_webz.py\nimport typing\n\nfrom flytekit import task\nfrom flytekit.types.file import FlyteFile\n\nfrom collectors.datatypes.document_models import Attribute, DataWarehouseDocument, WebZPost\nfrom collectors.mapping.mapping import (\n    convert_to_timestamp,\n    get_country_code,\n    get_country_from_domain,\n    get_iso_language_code,\n)\nfrom collectors.utils import flyte_file_to_input, output_to_flyte_file\n\n\n@task\ndef transform_webz(\n    posts: typing.List[FlyteFile],\n    bucket_url: str,\n    collector_run_name: str,\n    collector_id: str,\n) -> typing.List[typing.Dict[str, FlyteFile | str]]:\n    \"\"\"Transform collected documents from files in s3 bucket and prepare standard attributes.\"\"\"\n    print(\"Processing webz files into documents...\")\n\n    output_files: typing.List[typing.Dict[str, FlyteFile | str]] = []\n    file_prefix = f\"transform/{collector_run_name}\"\n\n    webzposts = [post for post in posts if post.path.endswith(\".meta\")]\n\n    for webzpost in webzposts:\n        webz = flyte_file_to_input(webzpost, WebZPost)\n        # Should only be None for tests. Set it to \"\" in this case to not fail the unit tests.\n        remote_source = \"\" if webzpost.remote_source is None else webzpost.remote_source\n\n        # Create a document from the collectors post\n        original_language = get_iso_language_code(webz.language)\n        geographic_domain = get_country_from_domain(webz.thread[\"site\"])\n        geographic_focus = get_country_code(webz.thread.get(\"country\", \"\"))\n        print(webz.thread.get(\"country\", \"\"))\n        print(geographic_focus)\n        published_date_timestamp = convert_to_timestamp(webz.thread[\"published\"])\n\n        # Skip any documents that have an invalid \"zero\" publication date\n        if published_date_timestamp.seconds != 0:\n            attributes = [\n                Attribute(key=\"publisher\", value_text=\"webz\"),\n                Attribute(key=\"aggregator_reference\", value_text=f\"WEBZ:{webz.thread['uuid']}\"),\n                Attribute(\n                    key=\"title\", value_text=webz.thread[\"title\"] or f\"Untitled document from {webz.thread['site']}\"\n                ),\n                Attribute(key=\"source_domain\", value_text=webz.thread[\"site\"]),\n                Attribute(key=\"published_date\", value_timestamp=published_date_timestamp),\n                Attribute(key=\"collected_file_url\", value_text=remote_source),\n                Attribute(key=\"collector_ids\", value_text_array=[collector_id]),\n                Attribute(key=\"original_location\", value_text=webz.thread[\"url\"]),\n            ]\n\n            if original_language:\n                attributes.append(Attribute(key=\"original_language\", value_text=original_language))\n\n            if \"site_categories\" in webz.thread and len(webz.thread[\"site_categories\"]) > 0:\n                # Remove any empty categories from the list\n                categories = [s for s in webz.thread[\"site_categories\"] if s.strip() != \"\"]\n                if len(categories) > 0:\n                    attributes.append(Attribute(key=\"topic\", value_text_array=categories))\n\n            if geographic_domain:\n                attributes.append(Attribute(key=\"geographic_domain\", value_text=geographic_domain))\n\n            if geographic_focus:\n                attributes.append(Attribute(key=\"geographic_focus\", value_text=geographic_focus))\n\n            if original_language != \"en\":\n                attributes.append(Attribute(key=\"translated\", value_text=\"true\"))\n            else:\n                attributes.append(Attribute(key=\"translated\", value_text=\"false\"))\n\n            document = DataWarehouseDocument(source_aggregator=\"WEBZ\", original_text=webz.text, attributes=attributes)\n            flyte_file = output_to_flyte_file(\n                document,\n                f\"{webz.thread['uuid']}.json\",\n                file_prefix=file_prefix,\n                bucket_url=bucket_url,\n            )\n\n            output_files.append({\"crawled\": webz.crawled, \"flyte_file\": flyte_file})\n\n    return output_files\n```\n\n\n\n```py data-warehouse-workflows/main/collectors/telemetry.py\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n)\nfrom opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter\n\nfrom collectors.config import Config\n\n\ndef setup_telemetry(settings: Config):\n    \"\"\"\n    Sets up OpenTelemetry tracing for the application.\n\n    This function configures thekOpenTelemetry TracerProvider with a resource\n    that includes the service name from the settings. It also sets up a\n    BatchSpanProcessor with an OTLPSpanExporter to export spans.\n\n    Args:\n        settings (Settings): The application settings containing the service name.\n    \"\"\"\n    resource = Resource(attributes={SERVICE_NAME: settings.APP_NAME})\n\n    provider = TracerProvider(resource=resource)\n\n    if settings.IS_TEST:\n        exporter = InMemorySpanExporter()\n    else:\n        exporter = OTLPSpanExporter()\n\n    processor = BatchSpanProcessor(exporter)\n    provider.add_span_processor(processor)\n\n    # Sets the global default tracer provider\n    trace.set_tracer_provider(provider)\n```\n\n```py data-warehouse-workflows/main/collectors/utils.py\nimport json\nimport os\nimport time\nimport unicodedata\nfrom http import HTTPStatus\nfrom typing import Any, TypeVar\nfrom urllib.parse import urlparse\n\nimport feedparser\nimport flytekit\nimport requests\nimport webzio\nfrom bs4 import BeautifulSoup, ParserRejectedMarkup\nfrom flytekit.exceptions import user\nfrom flytekit.types.file import FlyteFile\nfrom pydantic import BaseModel\nfrom scrapingbee import ScrapingBeeClient\n\nfrom custom_exceptions.custom_exceptions import InvalidFeedError\n\nT = TypeVar(\"T\")\nMAX_RETRIES = 5\n\n\ndef get_scrapingbee_remaining_quota(token: str):\n    try:\n        response = requests.get(\n            url=\"https://app.scrapingbee.com/api/v1/usage\",\n            params={\n                \"api_key\": token,\n            },\n        )\n\n        if response.status_code == HTTPStatus.OK:\n            content = json.loads(response.content.decode(\"utf-8\"))\n            return response.status_code, content[\"max_api_credit\"] - content[\"used_api_credit\"]\n    except Exception as e:\n        error_message = str(e)\n        redacted_message = error_message.replace(token, \"[REDACTED]\")\n        print(\n            JsonLog(\n                msg=\"Failed to get scraping bee quota.\",\n                error=redacted_message,\n            )\n        )\n\n    return response.status_code, 0\n\n\ndef scrape_content(url: str, token: str):\n    params = {\n        \"json_response\": True,\n        \"block_ads\": True,\n        \"wait_browser\": \"load\",\n        # only set if it is a google alert feed item otherwise non-google feed urls fail\n        \"custom_google\": \"google.com\" in urlparse(url).hostname,\n    }\n\n    client = ScrapingBeeClient(api_key=token)\n\n    retry_count = 0\n    while retry_count < MAX_RETRIES:\n        response = client.get(url, params=params)\n        # retry for specific status codes that are considered retryable with exponential backoff\n        if response.status_code in {500, 502, 503, 504, 429}:\n            retry_count += 1\n            time.sleep(2**retry_count)\n        else:\n            break\n\n    try:\n        # set errors to 'ignore' so that we can still extract content in the event the scraped web page has an invalid byte\n        # or has a mix of encodings causing the decode step to fail\n        content = json.loads(response.content.decode(\"utf-8\", errors=\"ignore\"))\n    except json.JSONDecodeError:\n        raise user.FlyteRecoverableException(f\"unable to decode content, status code: {response.status_code}\") from None\n\n    if response.status_code == HTTPStatus.OK:\n        resolved_url = content[\"resolved-url\"]\n        body = content[\"body\"]\n        return resolved_url, clean_text(body)\n    else:\n        # scrapingbee changes its response style for errors so handle both cases - we either have `{\"errors\": {...}}` or `{\"body\": {\"error\": \"\", \"reason\": \"\"}}`\n        if content.get(\"body\"):\n            body = content[\"body\"]\n            errors = body.get(\"error\", \"no errors found in body response\")\n            reason = body.get(\"reason\", \"no reason given in body response\")\n        else:\n            errors = content.get(\"errors\", \"no errors found in scraped response\")\n            reason = \"no reason given in response\"\n\n        raise user.FlyteRecoverableException(\n            f\"reason: {reason}, code: {response.status_code}, url: {url}, errors: {errors}\"\n        )\n\n\ndef fetch_rss_posts(\n    feed_url: str,\n    last_successful_iteration: time.struct_time,\n):\n    feed = feedparser.parse(feed_url)\n\n    # ignore character encoding orverride errors if `entries` exists and is not an empty list as the feed was still successfully parsed\n    if feed.bozo != 0 and not (\n        isinstance(feed.bozo_exception, feedparser.CharacterEncodingOverride) and feed.get(\"entries\")\n    ):\n        raise InvalidFeedError(f\"malformed rss/atom feed: {feed.bozo_exception}\")\n\n    missing_date_count = 0\n    skipped_count = 0\n    entries = feed[\"entries\"]\n\n    new_entries = {}\n    for entry in entries:\n        entry_time = None\n        # the parsed values are already forced to UTC\n        if entry.get(\"published_parsed\"):\n            entry_time = entry.get(\"published_parsed\")\n        elif entry.get(\"updated_parsed\"):\n            entry_time = entry.get(\"updated_parsed\")\n        else:\n            missing_date_count += 1\n            continue\n\n        if entry_time <= last_successful_iteration:\n            skipped_count += 1\n            continue\n\n        new_entries[entry_time] = entry\n\n    # ensures newer items to be ingested last\n    sorted_new_entries = dict(sorted(new_entries.items()))\n\n    total_results = len(sorted_new_entries.keys())\n\n    print(\n        JsonLog(\n            msg=\"rss articles\",\n            new=total_results,\n            skipped=skipped_count,\n            missing_date=missing_date_count,\n        )\n    )\n\n    return sorted_new_entries, total_results\n\n\ndef fetch_posts(\n    query: str,\n    params: dict,\n    batch_count: int,\n    token: str,\n):\n    webzio.config(token=token)\n    params.update({\"q\": query, \"sort\": \"crawled\"})\n    output = webzio.query(\"filterWebContent\", params)\n\n    total_results = output[\"totalResults\"]\n    requests_left = output[\"requestsLeft\"]\n\n    posts = output[\"posts\"]\n    count = 1\n    for _ in range(count, batch_count):\n        output = webzio.get_next()\n        requests_left = output[\"requestsLeft\"]\n        if len(output[\"posts\"]) > 0:\n            posts.extend(output[\"posts\"])\n        else:\n            print(\"No more posts\")\n            break\n        count += 1\n\n    if count == batch_count:\n        print(f\"Batch count reached ({batch_count})\")\n\n    return posts, total_results, requests_left\n\n\ndef raw_file_to_flyte(post: str, filename: str, file_prefix: str, bucket_url=None):\n    out_path = make_local_dir(file_prefix, filename)\n\n    with open(out_path, \"w\") as f:\n        f.write(post.encode(\"utf-8\"))  # save data to local file and ensure utf-8 encoding\n        if bucket_url is not None:\n            return FlyteFile(path=out_path, remote_path=os.path.join(bucket_url, f\"{file_prefix}/{filename}\"))\n        else:\n            return FlyteFile(path=out_path)\n\n\ndef make_local_dir(file_prefix, filename):\n    dir_path = os.path.join(f\"{flytekit.current_context().working_directory}\", f\"{file_prefix}\")\n    out_path = os.path.join(\n        f\"{dir_path}\",\n        f\"{filename}\",\n    )\n    os.makedirs(dir_path, exist_ok=True)\n    return out_path\n\n\ndef flyte_file_to_input(file: FlyteFile, data_type: type[T]) -> T:\n    \"\"\"Downloads a FlyteFile and loads it to a specified dataclass.\n\n    :param file: FlyteFile to download\n    :param data_type: Dataclass to load the file to\n    :return: Dataclass with the contents of the file\"\"\"\n    with file.open(\"r\") as f:\n        data = json.load(f)\n\n    return data_type(**data)\n\n\ndef output_to_flyte_file(dataclass: Any, filename: str, file_prefix: str, bucket_url=None) -> FlyteFile:\n    \"\"\"Saves a dataclass as a FlyteFile.\n\n    :param dataclass: Dataclass to save\n    :return: FlyteFile containing the dataclass\"\"\"\n    if not isinstance(dataclass, BaseModel):\n        raise TypeError(f\"Expected Pydantic BaseModel, got {type(dataclass)}\")\n\n    out_path = make_local_dir(file_prefix, filename)\n\n    with open(out_path, \"w\") as temp_file:\n        json.dump(dataclass.model_dump(exclude_none=True), temp_file)\n        temp_file.close()\n        if bucket_url is not None:\n            return FlyteFile(path=out_path, remote_path=os.path.join(bucket_url, f\"{file_prefix}/{filename}\"))\n        else:\n            return FlyteFile(path=out_path)\n\n\nclass JsonLog:\n    \"\"\"Given key value pairs as kwargs, provides a JSON style string that can be used for structured logging.\"\"\"\n\n    def __init__(self, /, **kwargs):\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return json.dumps(self.kwargs)\n\n\ndef is_graphic_or_space(r):\n    category = unicodedata.category(r)[0]\n    return category in (\"L\", \"M\", \"N\", \"P\", \"S\", \"Z\")\n\n\ndef clean_text(text: str):\n    # extract content\n    try:\n        extracted_text = BeautifulSoup(text, \"html.parser\").get_text(separator=\"\\n\").strip()\n    # sometimes the text may include items that the html.parser can't handle such as non standard or malformed tags\n    except ParserRejectedMarkup:\n        extracted_text = text\n\n    # remove unprintable unicode characters\n    cleaned_text = \"\".join(c if is_graphic_or_space(c) else \"\" for c in extracted_text)\n\n    return cleaned_text\n```\n\n\n```py data-warehouse-workflows/main/collectors/tasks/clean_documents.py\nimport logging\nimport typing\n\nfrom flytekit import FlyteFile, task\n\nfrom collectors.config import get_config\nfrom collectors.datatypes.document_models import DataLakeMetadata\nfrom collectors.tasks.text_cleaning.cleaning import clean_document_text\nfrom collectors.utils import flyte_file_to_input, raw_file_to_flyte\n\nconfig = get_config()\nlogger = logging.getLogger()\n\nfile_prefix = \"data-warehouse\"\n\n\n@task(pod_template_name=\"data-warehouse-pod-template\")\ndef clean_documents(\n    documents: typing.List[FlyteFile], metadatas: typing.List[FlyteFile], bucket_url: str\n) -> typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile]]:\n    \"\"\"\n    Tries to clean the documents.  Returns the original if it fails\n\n    Args:\n        documents: typing.List[FlyteFile] - typing.List of the document contents\n        metadatas: typing.List[FlyteFile] - typing.List of the document metadata\n        bucket_url: str - the bucket url to store the translated documents\n    Returns: typing.Tuple[typing.List[FlyteFile], typing.List[FlyteFile]]:\n        translated_documents - translated raw txt documents as FlyteFiles\n        updated_metadatas - updated metadata as FlyteFiles\n    \"\"\"\n\n    # Iterate the documents and clean each\n    cleaned_documents = []\n    updated_metadatas = []\n    for i, (document, meta_file) in enumerate(zip(documents, metadatas)):\n        # Convert the metadata flytefile to a dataclass\n        metadata: DataLakeMetadata = flyte_file_to_input(meta_file, DataLakeMetadata)\n        logger.info(f\"[cleaning] Cleaning document {i} with uuid {metadata.document_id} of {len(documents)}\")\n\n        with document.open(\"r\") as file:\n            text = str(file.read())\n\n        # Perform text cleaning, return the input if it fails\n        try:\n            cleaned_text = clean_document_text(text)\n        except Exception as e:\n            logger.error(\n                f\"[cleaning] Error cleaning text for document {metadata.document_id}: {e} - returning original text without cleaning\"\n            )\n            clean_documents.append(document)\n            break\n\n        # Store the result back in the datalake\n        text_file = raw_file_to_flyte(\n            cleaned_text,\n            \"cleaned.txt\",\n            file_prefix=f\"{file_prefix}/{metadata.document_id}\",\n            bucket_url=bucket_url,\n        )\n\n        cleaned_documents.append(text_file)\n\n        # Note: We don't update the metdata but could add something like cleaned: true\n\n    return cleaned_documents, updated_metadatas\n```\n\n\nSure, I can help you write tests for the `clean_documents` function. However, I'll need to make some assumptions about what the `clean_documents` function does. Typically, such a function might remove punctuation, convert text to lowercase, remove stopwords, etc. \n\nHere's an example of how you might write tests for the `clean_documents` function in a `test_clean_documents.py` file:\n\n```python\nimport unittest\nfrom your_module import clean_documents  # Replace 'your_module' with the actual module name\n\nclass TestCleanDocuments(unittest.TestCase):\n\n    def test_clean_documents_removes_punctuation(self):\n        input_docs = [\"Hello, world!\", \"This is a test.\"]\n        expected_output = [\"hello world\", \"this is a test\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_converts_to_lowercase(self):\n        input_docs = [\"HELLO WORLD\", \"This Is A Test\"]\n        expected_output = [\"hello world\", \"this is a test\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_removes_stopwords(self):\n        input_docs = [\"This is a test\", \"Hello world\"]\n        expected_output = [\"test\", \"hello world\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_handles_empty_strings(self):\n        input_docs = [\"\", \"Hello world\"]\n        expected_output = [\"\", \"hello world\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_handles_mixed_content(self):\n        input_docs = [\"Hello, WORLD!\", \"This is a TEST.\", \"123 numbers\"]\n        expected_output = [\"hello world\", \"test\", \"123 numbers\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_handles_non_ascii_characters(self):\n        input_docs = [\"Hllo, wrld!\", \"Ths s  tst.\"]\n        expected_output = [\"hllo wrld\", \"ths s  tst\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_handles_multiple_spaces(self):\n        input_docs = [\"Hello   world\", \"This  is  a  test\"]\n        expected_output = [\"hello world\", \"this is a test\"]\n        self.assertEqual(clean_documents(input_docs), expected_output)\n\n    def test_clean_documents_handles_numbers(self):\n        input_docs = [\"Hello 123\", \"Test 456\"]\n        expected_output = [\"hello 123\",\nWrite tests in the test_clean_documents.py for the clean_documents task in a similar way to the test_clean_documents function.\nTo write tests for the `clean_documents` task in a similar way to the `test_clean_documents` function, we need to create a test file named `test_clean_documents.py`. This file will contain unit tests that verify the functionality of the `clean_documents` task.\n\nHere's an example of how you can write these tests:\n\n```python\nimport os\nimport tempfile\nimport typing\nimport unittest\nfrom unittest.mock import patch, MagicMock\n\nfrom flytekit import FlyteFile\n\nfrom collectors.datatypes.document_models import DataLakeMetadata\nfrom collectors.tasks.clean_documents import clean_documents\nfrom collectors.tasks.text_cleaning.cleaning import clean_document_text\nfrom collectors.utils import raw_file_to_flyte, flyte_file_to_input\n\nclass TestCleanDocuments(unittest.TestCase):\n\n    def setUp(self):\n        # Create temporary directories for testing\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.bucket_url = \"s3://fake-bucket-url\"\n        self.file_prefix = \"data-warehouse\"\n\n    def tearDown(self):\n        # Clean up temporary directories\n        self.temp_dir.cleanup()\n\n    def create_temp_file(self, content: str, filename: str) -> FlyteFile:\n        file_path = os.path.join(self.temp_dir.name, filename)\n        with open(file_path, \"w\") as f:\n            f.write(content)\n        return FlyteFile(path=file_path)\n\n    @patch(\"collectors.tasks.clean_documents.clean_document_text\")\n    def test_clean_documents_success(self, mock_clean_document_text):\n        # Mock the clean_document_text function\n        mock_clean_document_text.side_effect = lambda text: f\"cleaned_{text}\"\n\n        # Create test documents and metadata\n        documents = [self.create_temp_file(\"test document content\", \"document1.txt\")]\n        metadatas = [self.create_temp_file('{\"document_id\": \"doc1\", \"attributes\": []}', \"metadata1.json\")]\n\n        # Call the clean_documents task\n        cleaned_documents, updated_metadatas = clean_documents(documents=documents, metadatas=metadatas, bucket_url=self.bucket_url)\n\n        # Verify the cleaned documents\n        self.assertEqual(len(cleaned_documents), 1)\n        with cleaned_documents[0].open() as f:\n            self.assertEqual(f.read(), \"cleaned_test document content\")\n\n        # Verify the updated metadata\n        self.assertEqual(len(updated_metadatas), 0)  # No updates to metadata in this example\n\n    @patch(\"collectors.tasks.clean_documents.clean_document_text\")\n    def test_clean_documents_failure(self, mock_clean_document_text):\n        # Mock the clean_document_text function to raise an exception\n        mock_clean_document_text.side_effect = Exception(\"Cleaning error\")\n\n        # Create test documents and metadata\n        documents = [self.create_temp_file(\"test document content\", \"document1.txt\")]\n        metadatas = [self.create_temp_file('{\"document_id\": \"doc1\", \"attributes\": []}', \"metadata1.json\")]\n\n        # Call the clean_documents task\n        cleaned_documents, updated_metadatas = clean_documents(documents=documents, metadatas=metadatas, bucket_url=self.bucket_url)\n\n        # Verify the cleaned documents (should return the original document)\n        self.assertEqual(len(cleaned_documents), 1)\n        with cleaned_documents[0].open() as f:\n            self.assertEqual(f.read(), \"test document content\")\n\n        # Verify the updated metadata\n        self.assertEqual(len(updated_metadatas), 0)  # No updates to metadata in this example\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\nIn this example, we have created a test class `TestCleanDocuments` that contains two test methods:\n\n1. `test_clean_documents_success`: This test verifies that the `clean_documents` task successfully cleans the document content using the `clean_document_text` function. It mocks the `clean_document_text` function to return a cleaned version of the input text.\n\n2. `test_clean_documents_failure`: This test verifies that the `clean_documents` task handles exceptions raised by the `clean_document_text` function. It mocks the `clean_document_text` function to raise an exception and checks that the original document content is returned.\n\nBoth tests use the `setUp` and `tearDown` methods to create and clean up temporary directories for testing. The `create_temp_file` helper method is used to create temporary files for the test documents and metadata.\n\nYou can run these tests using a test runner like `unittest` by executing the script.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":997}}},{"id":{"replica_id":0,"value":18},"start":99,"metadata":{"role":"assistant","status":"Canceled","timestamp":{"replica_id":0,"value":998}}},{"id":{"replica_id":0,"value":19},"start":2038,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":999}}},{"id":{"replica_id":0,"value":165},"start":149865,"metadata":{"role":"assistant","status":"Canceled","timestamp":{"replica_id":0,"value":1000}}},{"id":{"replica_id":0,"value":166},"start":152282,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1001}}},{"id":{"replica_id":0,"value":483},"start":152409,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":1002}}},{"id":{"replica_id":0,"value":484},"start":156829,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":1003}}}],"summary":"Writing Tests for clean_documents Task","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null},{"range":{"start":2165,"end":147280},"icon":"Folder","label":"data-warehouse-workflows/main/collectors","metadata":null},{"range":{"start":2206,"end":2480},"icon":"File","label":"data-warehouse-workflows/main/collectors/__init__.py","metadata":{"path":"data-warehouse-workflows/main/collectors/__init__.py"}},{"range":{"start":2481,"end":5469},"icon":"File","label":"data-warehouse-workflows/main/collectors/collector_rss_workflow.py","metadata":{"path":"data-warehouse-workflows/main/collectors/collector_rss_workflow.py"}},{"range":{"start":5470,"end":9051},"icon":"File","label":"data-warehouse-workflows/main/collectors/collector_webz_workflow.py","metadata":{"path":"data-warehouse-workflows/main/collectors/collector_webz_workflow.py"}},{"range":{"start":9052,"end":10779},"icon":"File","label":"data-warehouse-workflows/main/collectors/config.py","metadata":{"path":"data-warehouse-workflows/main/collectors/config.py"}},{"range":{"start":10780,"end":14860},"icon":"File","label":"data-warehouse-workflows/main/collectors/custom_logging.py","metadata":{"path":"data-warehouse-workflows/main/collectors/custom_logging.py"}},{"range":{"start":14861,"end":16256},"icon":"Folder","label":"datatypes","metadata":null},{"range":{"start":14871,"end":14945},"icon":"File","label":"data-warehouse-workflows/main/collectors/datatypes/__init__.py","metadata":{"path":"data-warehouse-workflows/main/collectors/datatypes/__init__.py"}},{"range":{"start":14946,"end":16255},"icon":"File","label":"data-warehouse-workflows/main/collectors/datatypes/document_models.py","metadata":{"path":"data-warehouse-workflows/main/collectors/datatypes/document_models.py"}},{"range":{"start":16257,"end":70683},"icon":"Folder","label":"mapping","metadata":null},{"range":{"start":16265,"end":18439},"icon":"File","label":"data-warehouse-workflows/main/collectors/mapping/mapping.py","metadata":{"path":"data-warehouse-workflows/main/collectors/mapping/mapping.py"}},{"range":{"start":18440,"end":70682},"icon":"File","label":"data-warehouse-workflows/main/collectors/mapping/source_mapping_source_groups_v5.csv","metadata":{"path":"data-warehouse-workflows/main/collectors/mapping/source_mapping_source_groups_v5.csv"}},{"range":{"start":70684,"end":74879},"icon":"File","label":"data-warehouse-workflows/main/collectors/metrics.py","metadata":{"path":"data-warehouse-workflows/main/collectors/metrics.py"}},{"range":{"start":74880,"end":76638},"icon":"File","label":"data-warehouse-workflows/main/collectors/process_run.py","metadata":{"path":"data-warehouse-workflows/main/collectors/process_run.py"}},{"range":{"start":76639,"end":137549},"icon":"Folder","label":"tasks","metadata":null},{"range":{"start":76645,"end":76714},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/__init__.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/__init__.py"}},{"range":{"start":76715,"end":79297},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/clean_documents.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/clean_documents.py"}},{"range":{"start":79298,"end":80351},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/get_documents_from_collector.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/get_documents_from_collector.py"}},{"range":{"start":80352,"end":82090},"icon":"Folder","label":"langdetect","metadata":null},{"range":{"start":80363,"end":82089},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/langdetect/lingua_detection.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/langdetect/lingua_detection.py"}},{"range":{"start":82091,"end":88471},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/load_document_to_warehouse.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/load_document_to_warehouse.py"}},{"range":{"start":88472,"end":93212},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/load_translation_to_warehouse.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/load_translation_to_warehouse.py"}},{"range":{"start":93213,"end":102046},"icon":"Folder","label":"rss","metadata":null},{"range":{"start":93217,"end":93290},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/rss/__init__.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/rss/__init__.py"}},{"range":{"start":93291,"end":98948},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/rss/extract_from_rss.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/rss/extract_from_rss.py"}},{"range":{"start":98949,"end":102045},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/rss/transform_rss.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/rss/transform_rss.py"}},{"range":{"start":102047,"end":102469},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/send_batch_to_inference.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/send_batch_to_inference.py"}},{"range":{"start":102470,"end":111558},"icon":"Folder","label":"text_cleaning","metadata":null},{"range":{"start":102484,"end":111173},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/text_cleaning/cleaning.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/text_cleaning/cleaning.py"}},{"range":{"start":111174,"end":111557},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/text_cleaning/sentencizer.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/text_cleaning/sentencizer.py"}},{"range":{"start":111559,"end":112702},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/transform_documents.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/transform_documents.py"}},{"range":{"start":112703,"end":122800},"icon":"Folder","label":"translate","metadata":null},{"range":{"start":112713,"end":116344},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/translate/chunking.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/translate/chunking.py"}},{"range":{"start":116345,"end":122799},"icon":"Folder","label":"providers","metadata":null},{"range":{"start":116355,"end":121674},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/translate/providers/llm_provider.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/translate/providers/llm_provider.py"}},{"range":{"start":121675,"end":122798},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/translate/providers/provider.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/translate/providers/provider.py"}},{"range":{"start":122801,"end":129932},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/translate_documents.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/translate_documents.py"}},{"range":{"start":129933,"end":137548},"icon":"Folder","label":"webz","metadata":null},{"range":{"start":129938,"end":130012},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/webz/__init__.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/webz/__init__.py"}},{"range":{"start":130013,"end":133486},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/webz/extract_from_webz.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/webz/extract_from_webz.py"}},{"range":{"start":133487,"end":137547},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/webz/transform_webz.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/webz/transform_webz.py"}},{"range":{"start":137550,"end":138874},"icon":"File","label":"data-warehouse-workflows/main/collectors/telemetry.py","metadata":{"path":"data-warehouse-workflows/main/collectors/telemetry.py"}},{"range":{"start":138875,"end":147279},"icon":"File","label":"data-warehouse-workflows/main/collectors/utils.py","metadata":{"path":"data-warehouse-workflows/main/collectors/utils.py"}},{"range":{"start":147281,"end":149863},"icon":"File","label":"data-warehouse-workflows/main/collectors/tasks/clean_documents.py","metadata":{"path":"data-warehouse-workflows/main/collectors/tasks/clean_documents.py"}}]}